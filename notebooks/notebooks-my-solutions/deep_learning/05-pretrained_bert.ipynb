{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f405861c",
   "metadata": {},
   "source": [
    "# BERT\n",
    "\n",
    "**OBJETIVO:** Ao final desta aula, você será capaz de **usar um BERT pré‑treinado** para:  \n",
    "1. Gerar **sugestões** (por exemplo, preencher lacunas de texto ou completar sentenças mascaradas).  \n",
    "2. Extrair **embeddings contextuais** para tarefas de classificação.\n",
    "\n",
    "> **Explicação aprofundada:**  \n",
    "> - **BERT** (Bidirectional Encoder Representations from Transformers) é um modelo de linguagem baseado em Transformer, pré‑treinado em grandes corpora usando:\n",
    ">   - **Masked Language Modeling** (MLM): algumas palavras são ocultadas e o modelo aprende a prever as palavras mascaradas.  \n",
    ">   - **Next Sentence Prediction** (NSP): o modelo aprende a relacionar sentenças em sequência.\n",
    "> - Ao usar um BERT pré‑treinado:\n",
    ">   1. **Tokenizer**: converte texto em IDs de sub‑palavras (WordPieces).  \n",
    ">   2. **Modelo**: responde tarefas como “fill‑mask” ou retorna um tensor de embeddings para cada token, que podemos agregar (p.ex., via média ou usando o token `[CLS]`) para alimentar um classificador.\n",
    "> - **Sugestões** (a): podemos usar a função de preenchimento de máscara para completar palavras ou frases de forma natural.  \n",
    "> - **Embeddings** (b): extraímos vetores que capturam o contexto de cada token/sentença e servem como **features** poderosas em modelos de classificação downstream."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2582c6fd",
   "metadata": {},
   "source": [
    "## O que é o BERT?\n",
    "\n",
    "Após o surgimento do [Transformer](https://arxiv.org/abs/1706.03762) — que revolucionou o processamento de sequências ao introduzir o mecanismo de **atenção** em múltiplas cabeças, sem recorrer a arquiteturas recorrentes — vieram diversos avanços. Um deles foi o [GPT](https://paperswithcode.com/paper/improving-language-understanding-by), que adota **apenas o decodificador** do Transformer para realizar a tarefa de **previsão do próximo token**. Nesse esquema, o GPT usa máscaras unidirecionais na atenção (cada posição só “vê” o que vem **antes**), evitando previsões triviais e aprendendo a gerar continuations de texto coerentes. O resultado é um espaço de embeddings em que tokens plausíveis para a sequência aparecem com maior probabilidade.\n",
    "\n",
    "O time do Google propôs outra abordagem inovadora: em vez de decodificador, usar **apenas o encoder** do Transformer e treinar o modelo para duas tarefas simultâneas:\n",
    "\n",
    "1. **Masked Language Modeling (MLM)**  \n",
    "   - Algumas palavras (ou subpalavras) são **mascaradas** (substituídas por um token especial `[MASK]`) durante o pré‑treino.  \n",
    "   - O modelo deve **predizer a palavra mascarada** usando informações de **todo** o contexto — tanto o que vem antes **quanto** o que vem depois.  \n",
    "   - Exemplo: em “Luke, I am your `[MASK]`”, o BERT observa “Luke, I am your ____” e usa os dois lados para preencher o espaço em branco.\n",
    "\n",
    "2. **Next Sentence Prediction (NSP)**  \n",
    "   - Dada uma dupla de sentenças \\((A, B)\\), o modelo aprende a classificar se **B realmente segue A** no corpus original.  \n",
    "   - Isso reforça a capacidade de capturar relações de sequência entre frases, útil em tarefas como pergunta‑resposta e inferência textual.\n",
    "\n",
    "Com isso, nasceu o **BERT** (Bidirectional Encoder Representations from Transformers).  \n",
    "> **Por que “bidirecional”?**  \n",
    "> Porque, ao contrário de modelos unidirecionais (como o GPT), o BERT aproveita **ambos os lados** do contexto em cada posição, enriquecendo a representação de cada token.\n",
    "\n",
    "Ao final do pré‑treino, temos um modelo capaz de:\n",
    "- **Gerar sugestões** precisas para completar trechos de texto.  \n",
    "- **Extrair embeddings** contextuais (por exemplo, via o vetor do token `[CLS]`) para alimentar classificadores em tarefas downstream, como classificação de sentimentos, perguntas e respostas, detecção de entidades etc.  \n",
    "\n",
    "Esses recursos fazem do BERT a base de inúmeras aplicações de NLP de última geração."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7da52fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 12.2.1 (20241206.2353)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"625pt\" height=\"344pt\"\n",
       " viewBox=\"0.00 0.00 625.00 344.47\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 340.47)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-340.47 621,-340.47 621,4 -4,4\"/>\n",
       "<g id=\"clust1\" class=\"cluster\">\n",
       "<title>cluster_input</title>\n",
       "<path fill=\"lightgrey\" stroke=\"lightgrey\" d=\"M20,-157.98C20,-157.98 597,-157.98 597,-157.98 603,-157.98 609,-163.98 609,-169.98 609,-169.98 609,-316.47 609,-316.47 609,-322.47 603,-328.47 597,-328.47 597,-328.47 20,-328.47 20,-328.47 14,-328.47 8,-322.47 8,-316.47 8,-316.47 8,-169.98 8,-169.98 8,-163.98 14,-157.98 20,-157.98\"/>\n",
       "<text text-anchor=\"middle\" x=\"308.5\" y=\"-311.17\" font-family=\"Times,serif\" font-size=\"14.00\">Input</text>\n",
       "</g>\n",
       "<!-- T -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>T</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"98\" cy=\"-266.97\" rx=\"82.06\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"98\" y=\"-261.92\" font-family=\"Times,serif\" font-size=\"14.00\">Token embeddings</text>\n",
       "</g>\n",
       "<!-- ADD -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>ADD</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"287\" cy=\"-183.98\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"287\" y=\"-178.93\" font-family=\"Times,serif\" font-size=\"14.00\">+</text>\n",
       "</g>\n",
       "<!-- T&#45;&gt;ADD -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>T&#45;&gt;ADD</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M133.99,-250.55C168.79,-235.64 221.02,-213.26 254.56,-198.88\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"255.54,-202.27 263.35,-195.12 252.78,-195.84 255.54,-202.27\"/>\n",
       "</g>\n",
       "<!-- P -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>P</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"287\" cy=\"-266.97\" rx=\"88.71\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"287\" y=\"-261.92\" font-family=\"Times,serif\" font-size=\"14.00\">Position embeddings</text>\n",
       "</g>\n",
       "<!-- P&#45;&gt;ADD -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>P&#45;&gt;ADD</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M287,-248.8C287,-238.67 287,-225.58 287,-213.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"290.5,-213.93 287,-203.93 283.5,-213.93 290.5,-213.93\"/>\n",
       "</g>\n",
       "<!-- S -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>S</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"497\" cy=\"-266.97\" rx=\"103.59\" ry=\"28.99\"/>\n",
       "<text text-anchor=\"middle\" x=\"497\" y=\"-270.17\" font-family=\"Times,serif\" font-size=\"14.00\">Segment embeddings</text>\n",
       "<text text-anchor=\"middle\" x=\"497\" y=\"-253.67\" font-family=\"Times,serif\" font-size=\"14.00\">(indica sentença 1 ou 2)</text>\n",
       "</g>\n",
       "<!-- S&#45;&gt;ADD -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>S&#45;&gt;ADD</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M437.1,-242.87C399.26,-228.28 351.76,-209.96 320.45,-197.88\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"322.09,-194.77 311.5,-194.43 319.57,-201.3 322.09,-194.77\"/>\n",
       "</g>\n",
       "<!-- SEQ -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>SEQ</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"287\" cy=\"-111.98\" rx=\"73.36\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"287\" y=\"-106.93\" font-family=\"Times,serif\" font-size=\"14.00\">Sequence Model</text>\n",
       "</g>\n",
       "<!-- ADD&#45;&gt;SEQ -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>ADD&#45;&gt;SEQ</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M287,-165.68C287,-158.39 287,-149.71 287,-141.52\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"290.5,-141.6 287,-131.6 283.5,-141.6 290.5,-141.6\"/>\n",
       "</g>\n",
       "<!-- RES -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>RES</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"287\" cy=\"-28.99\" rx=\"120.03\" ry=\"28.99\"/>\n",
       "<text text-anchor=\"middle\" x=\"287\" y=\"-32.19\" font-family=\"Times,serif\" font-size=\"14.00\">Resultado:</text>\n",
       "<text text-anchor=\"middle\" x=\"287\" y=\"-15.69\" font-family=\"Times,serif\" font-size=\"14.00\">1 vetor por token de entrada</text>\n",
       "</g>\n",
       "<!-- SEQ&#45;&gt;RES -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>SEQ&#45;&gt;RES</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M287,-93.81C287,-86.72 287,-78.19 287,-69.7\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"290.5,-69.81 287,-59.81 283.5,-69.81 290.5,-69.81\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x10440b820>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "dot = Digraph(format='png')\n",
    "# Agrupamento “Input”\n",
    "with dot.subgraph(name='cluster_input') as c:\n",
    "    c.attr(label='Input', style='rounded,filled', color='lightgrey')\n",
    "    c.node('T', 'Token embeddings')\n",
    "    c.node('P', 'Position embeddings')\n",
    "    c.node('S', 'Segment embeddings\\n(indica sentença 1 ou 2)')\n",
    "    c.node('ADD', '+')\n",
    "    # Conexões dentro do subgraph\n",
    "    c.edge('T', 'ADD')\n",
    "    c.edge('P', 'ADD')\n",
    "    c.edge('S', 'ADD')\n",
    "\n",
    "# Fluxo principal\n",
    "dot.node('SEQ', 'Sequence Model')\n",
    "dot.node('RES', 'Resultado:\\n1 vetor por token de entrada')\n",
    "\n",
    "dot.edge('ADD', 'SEQ')\n",
    "dot.edge('SEQ', 'RES')\n",
    "\n",
    "# Exibir no Jupyter\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f483ee",
   "metadata": {},
   "source": [
    "Continuando:\n",
    "\n",
    "O nome **BERT** vem de **Bidirectional Encoder Representations from Transformers** e foi apresentado neste [artigo de 2019](https://arxiv.org/pdf/1810.04805). A grande inovação do BERT, além da arquitetura Transformer “somente encoder”, é o **pré‑treino multitarefa**, em que o modelo aprende simultaneamente:\n",
    "\n",
    "1. **Masked Language Modeling (MLM)** – prever palavras mascaradas usando contexto bidirecional.  \n",
    "2. **Next Sentence Prediction (NSP)** – classificar se duas sentenças realmente aparecem em sequência.\n",
    "\n",
    "Não vamos treinar o BERT do zero em aula (é muito pesado!), mas sim **utilizar um modelo pré‑treinado** para tarefas downstream. Faremos isso usando a implementação oficial do BERT pela Hugging Face:  \n",
    "> **bert-base-uncased**  \n",
    "> 📚 Documentação completa: https://huggingface.co/google-bert/bert-base-uncased\n",
    "\n",
    "---\n",
    "\n",
    "## Tarefa 1: Masked Language Model\n",
    "\n",
    "A primeira tarefa no pré‑treino do BERT foi o **Masked Language Model**, inspirada nos testes de “cloze” em linguística. A ideia é:\n",
    "\n",
    "- **Remover** (ou substituir por `[MASK]`) uma ou mais palavras numa frase.  \n",
    "- **Pedir** ao modelo que **preveja** qual é a palavra que falta, usando **todo** o contexto (antes **e** depois de `[MASK]`).  \n",
    "\n",
    "> **Exemplo de cloze**:  \n",
    "> > “Luke, I am your `[MASK]`.”  \n",
    ">  \n",
    "> O BERT vê `\"Luke, I am your __ .\"` e utiliza tanto as palavras anteriores (**Luke, I am your**) quanto as posteriores (neste caso, o ponto final) para descobrir que a resposta lógica é **“father”**.\n",
    "\n",
    "**Próximos passos**:  \n",
    "- Vamos carregar o tokenizer e o modelo pré‑treinado do Hugging Face.  \n",
    "- Construir um pipeline de preenchimento de máscara (`fill-mask`) para experimentar com frases mascaradas.  \n",
    "- Discutir como extrair as probabilidades de cada palavra candidata e interpretar os resultados.\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "# 1) Cria um pipeline de Masked LM usando o BERT pré‑treinado\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"google/bert-base-uncased\",\n",
    "    tokenizer=\"google/bert-base-uncased\"\n",
    ")\n",
    "\n",
    "# 2) Testa com uma frase mascarada\n",
    "resultado = fill_mask(\"Luke, I am your [MASK].\")\n",
    "for r in resultado:\n",
    "    print(f\"{r['token_str']}: probabilidade {r['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413b225e",
   "metadata": {},
   "source": [
    "Explicação aprofundada:\n",
    "- O pipeline de fill-mask já:\n",
    "\t1.\tTokeniza o texto, insertando [MASK].\n",
    "\t2.\tPassa pelo modelo BERT para obter logits em cada posição.\n",
    "\t3.\tAplica Softmax apenas sobre o vocabulário na posição mascarada.\n",
    "\t4.\tRetorna as top‑k palavras mais prováveis para substituir [MASK], junto com suas probabilidades.\n",
    "- Isso ilustra como o BERT usa o contexto bidirecional para preencher lacunas de forma muito mais precisa que modelos unidirecionais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c3e07ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "father: 0.2734\n",
      "brother: 0.1040\n",
      "mother: 0.1030\n",
      "friend: 0.0900\n",
      "son: 0.0722\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, pipeline\n",
    "\n",
    "# 1) Carregue manualmente o tokenizer e o modelo com local_files_only=True\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    local_files_only=True\n",
    ")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "# 2) Crie o pipeline passando os objetos já carregados\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# 3) Use normalmente\n",
    "resultados = fill_mask(\"Luke, I am your [MASK].\")\n",
    "for r in resultados:\n",
    "    print(f\"{r['token_str']}: {r['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c0dce8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 12.2.1 (20241206.2353)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"199pt\" height=\"582pt\"\n",
       " viewBox=\"0.00 0.00 198.75 582.31\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 578.31)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-578.31 194.75,-578.31 194.75,4 -4,4\"/>\n",
       "<g id=\"clust1\" class=\"cluster\">\n",
       "<title>cluster_inputs</title>\n",
       "<path fill=\"lightgrey\" stroke=\"lightgrey\" d=\"M53.38,-351.16C53.38,-351.16 137.38,-351.16 137.38,-351.16 143.38,-351.16 149.38,-357.16 149.38,-363.16 149.38,-363.16 149.38,-554.31 149.38,-554.31 149.38,-560.31 143.38,-566.31 137.38,-566.31 137.38,-566.31 53.38,-566.31 53.38,-566.31 47.38,-566.31 41.38,-560.31 41.38,-554.31 41.38,-554.31 41.38,-363.16 41.38,-363.16 41.38,-357.16 47.38,-351.16 53.38,-351.16\"/>\n",
       "<text text-anchor=\"middle\" x=\"95.38\" y=\"-549.01\" font-family=\"Times,serif\" font-size=\"14.00\">Inputs</text>\n",
       "</g>\n",
       "<g id=\"clust2\" class=\"cluster\">\n",
       "<title>cluster_outputs</title>\n",
       "<path fill=\"lightgrey\" stroke=\"lightgrey\" d=\"M72.38,-64C72.38,-64 118.38,-64 118.38,-64 124.38,-64 130.38,-70 130.38,-76 130.38,-76 130.38,-267.16 130.38,-267.16 130.38,-273.16 124.38,-279.16 118.38,-279.16 118.38,-279.16 72.38,-279.16 72.38,-279.16 66.38,-279.16 60.38,-273.16 60.38,-267.16 60.38,-267.16 60.38,-76 60.38,-76 60.38,-70 66.38,-64 72.38,-64\"/>\n",
       "<text text-anchor=\"middle\" x=\"95.38\" y=\"-261.86\" font-family=\"Times,serif\" font-size=\"14.00\">Outputs</text>\n",
       "</g>\n",
       "<!-- INPUT -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>INPUT</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"95.38\" cy=\"-446.48\" rx=\"45.79\" ry=\"87.33\"/>\n",
       "<text text-anchor=\"middle\" x=\"95.38\" y=\"-490.93\" font-family=\"Times,serif\" font-size=\"14.00\">[CLS]</text>\n",
       "<text text-anchor=\"middle\" x=\"95.38\" y=\"-474.43\" font-family=\"Times,serif\" font-size=\"14.00\">remove</text>\n",
       "<text text-anchor=\"middle\" x=\"95.38\" y=\"-457.93\" font-family=\"Times,serif\" font-size=\"14.00\">some</text>\n",
       "<text text-anchor=\"middle\" x=\"95.38\" y=\"-441.43\" font-family=\"Times,serif\" font-size=\"14.00\">parts</text>\n",
       "<text text-anchor=\"middle\" x=\"95.38\" y=\"-424.93\" font-family=\"Times,serif\" font-size=\"14.00\">[MASK]</text>\n",
       "<text text-anchor=\"middle\" x=\"95.38\" y=\"-408.43\" font-family=\"Times,serif\" font-size=\"14.00\">a</text>\n",
       "<text text-anchor=\"middle\" x=\"95.38\" y=\"-391.93\" font-family=\"Times,serif\" font-size=\"14.00\">sentence</text>\n",
       "</g>\n",
       "<!-- BERT -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>BERT</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"95.38\" cy=\"-305.16\" rx=\"34.46\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"95.38\" y=\"-300.11\" font-family=\"Times,serif\" font-size=\"14.00\">BERT</text>\n",
       "</g>\n",
       "<!-- INPUT&#45;&gt;BERT -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>INPUT&#45;&gt;BERT</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M95.38,-358.83C95.38,-350.18 95.38,-341.9 95.38,-334.57\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"98.88,-334.76 95.38,-324.76 91.88,-334.76 98.88,-334.76\"/>\n",
       "</g>\n",
       "<!-- OUTPUT -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>OUTPUT</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"95.38\" cy=\"-159.33\" rx=\"27\" ry=\"87.33\"/>\n",
       "<text text-anchor=\"middle\" x=\"95.38\" y=\"-203.78\" font-family=\"Times,serif\" font-size=\"14.00\">C</text>\n",
       "<text text-anchor=\"middle\" x=\"95.38\" y=\"-187.28\" font-family=\"Times,serif\" font-size=\"14.00\">T1</text>\n",
       "<text text-anchor=\"middle\" x=\"95.38\" y=\"-170.78\" font-family=\"Times,serif\" font-size=\"14.00\">T2</text>\n",
       "<text text-anchor=\"middle\" x=\"95.38\" y=\"-154.28\" font-family=\"Times,serif\" font-size=\"14.00\">T3</text>\n",
       "<text text-anchor=\"middle\" x=\"95.38\" y=\"-137.78\" font-family=\"Times,serif\" font-size=\"14.00\">T4</text>\n",
       "<text text-anchor=\"middle\" x=\"95.38\" y=\"-121.28\" font-family=\"Times,serif\" font-size=\"14.00\">T5</text>\n",
       "<text text-anchor=\"middle\" x=\"95.38\" y=\"-104.78\" font-family=\"Times,serif\" font-size=\"14.00\">T6</text>\n",
       "</g>\n",
       "<!-- BERT&#45;&gt;OUTPUT -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>BERT&#45;&gt;OUTPUT</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M95.38,-286.81C95.38,-279.04 95.38,-269.22 95.38,-258.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"98.88,-258.56 95.38,-248.56 91.88,-258.56 98.88,-258.56\"/>\n",
       "</g>\n",
       "<!-- TRAIN -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>TRAIN</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"184.75,-36 0,-36 0,0 190.75,0 190.75,-30 184.75,-36\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"184.75,-36 184.75,-30\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"190.75,-30 184.75,-30\"/>\n",
       "<text text-anchor=\"middle\" x=\"95.38\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">Loss: T4 should be the word &#39;of&#39;</text>\n",
       "</g>\n",
       "<!-- OUTPUT&#45;&gt;TRAIN -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>OUTPUT&#45;&gt;TRAIN</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" d=\"M95.38,-71.67C95.38,-63.02 95.38,-54.75 95.38,-47.42\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"98.88,-47.6 95.38,-37.6 91.88,-47.6 98.88,-47.6\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x140a67d60>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "# Cria o grafo no formato PNG\n",
    "dot = Digraph(format='png')\n",
    "\n",
    "# Subgrafo de Inputs\n",
    "with dot.subgraph(name='cluster_inputs') as c:\n",
    "    c.attr(label='Inputs', style='rounded,filled', color='lightgrey')\n",
    "    c.node('INPUT', '[CLS]\\nremove\\nsome\\nparts\\n[MASK]\\na\\nsentence')\n",
    "\n",
    "# Nó central: BERT\n",
    "dot.node('BERT', 'BERT')\n",
    "\n",
    "# Subgrafo de Outputs\n",
    "with dot.subgraph(name='cluster_outputs') as c:\n",
    "    c.attr(label='Outputs', style='rounded,filled', color='lightgrey')\n",
    "    c.node('OUTPUT', 'C\\nT1\\nT2\\nT3\\nT4\\nT5\\nT6')\n",
    "\n",
    "# Nó de Treinamento\n",
    "dot.node('TRAIN', 'Loss: T4 should be the word \\'of\\'', shape='note')\n",
    "\n",
    "# Conexões\n",
    "dot.edge('INPUT', 'BERT')\n",
    "dot.edge('BERT', 'OUTPUT')\n",
    "dot.edge('OUTPUT', 'TRAIN', style='dashed')\n",
    "\n",
    "# Exibe o diagrama\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39cb2ea",
   "metadata": {},
   "source": [
    "O diagrama mostra de forma esquemática como funciona uma etapa de **Masked Language Modeling** usando o BERT:\n",
    "\n",
    "1. **Cluster “Inputs”**  \n",
    "   - Contém a sequência de tokens que chega ao modelo:  \n",
    "     ```  \n",
    "     [CLS]  \n",
    "     remove  \n",
    "     some  \n",
    "     parts  \n",
    "     [MASK]  \n",
    "     a  \n",
    "     sentence  \n",
    "     ```  \n",
    "   - Aqui:  \n",
    "     - `[CLS]` é o token especial usado pelo BERT para sinalizar o início da entrada e cujo embedding pode ser usado em tarefas de classificação de sequência.  \n",
    "     - Cada palavra (sub‑palavra) do texto vira um token separado.  \n",
    "     - O token `[MASK]` substitui a palavra que queremos que o modelo aprenda a prever (“of”, neste exemplo).\n",
    "\n",
    "2. **Seta para “BERT”**  \n",
    "   - A seta indica que esses tokens são processados **simultaneamente** pelo encoder bidirecional do BERT.  \n",
    "   - Internamente, cada token é convertido em um vetor de embedding, somado aos embeddings de posição e de segmento, e então passa por várias camadas de atenção e feed‑forward.\n",
    "\n",
    "3. **Cluster “Outputs”**  \n",
    "   - Após o processamento, o BERT gera um vetor de saída para **cada** token da sequência de entrada:\n",
    "     ```  \n",
    "     C  \n",
    "     T1  \n",
    "     T2  \n",
    "     T3  \n",
    "     T4  \n",
    "     T5  \n",
    "     T6  \n",
    "     ```  \n",
    "   - Esses vetores representam, em dimensões altas, a informação contextualizada de cada posição.  \n",
    "   - Em um pipeline de “fill‑mask”, o vetor na posição de `[MASK]` (aqui, T4) é passado por uma camada linear + softmax para produzir uma distribuição de probabilidade sobre todo o vocabulário.\n",
    "\n",
    "4. **Seta tracejada até “Loss: T4 should be the word 'of'”**  \n",
    "   - A linha tracejada conecta a saída do token mascarado (T4) ao critério de perda.  \n",
    "   - Durante o treinamento, compara‑se o vetor T4 com a etiqueta “of” usando Cross‑Entropy Loss:  \n",
    "     $$\n",
    "       \\mathcal{L} = -\\log P(\\text{“of”}\\mid \\text{contexto})\n",
    "     $$  \n",
    "   - O objetivo é ajustar os pesos do BERT para que, dado o contexto completo, a probabilidade estimada para a palavra “of” seja a maior.\n",
    "\n",
    "---\n",
    "\n",
    "**Em resumo**, o diagrama ilustra:\n",
    "\n",
    "- Como a sequência de tokens de entrada, com um elemento mascarado, é injetada no BERT.  \n",
    "- Como o BERT produz uma saída vetorial para cada posição.  \n",
    "- Como se define uma função de perda que força o vetor correspondente à posição mascarada a identificar corretamente a palavra que falta.  \n",
    "\n",
    "Esse fluxo é a essência do **Masked Language Modeling**, permitindo que o BERT aprenda representações ricas e bidirecionais ao prever palavras ausentes usando todo o contexto ao redor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9324917e",
   "metadata": {},
   "source": [
    "### Exemplo de uso do pipeline `fill-mask` com BERT\n",
    "\n",
    "A seguir, usamos o pipeline de **Masked Language Modeling** (`fill-mask`) para preencher o token mascarado em uma frase:\n",
    "\n",
    "1. **Criação do pipeline**  \n",
    "   ```python\n",
    "   from transformers import pipeline\n",
    "   unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "\n",
    "\t2.\tAplicação\n",
    "Ao chamar:\n",
    "\n",
    "unmasker(\"Remove some parts [MASK] a sentence.\")\n",
    "\n",
    "o pipeline retorna uma lista (top‑5 por padrão) de possíveis substituições para [MASK], cada uma com:\n",
    "\t•\tscore: probabilidade estimada pelo modelo.\n",
    "\t•\ttoken: ID do token no vocabulário.\n",
    "\t•\ttoken_str: string do token previsto.\n",
    "\t•\tsequence: frase completa com o token substituído.\n",
    "\n",
    "Interpretação do resultado:\n",
    "\t•\tA melhor previsão para preencher [MASK] foi “of” com probabilidade ~0.9431.\n",
    "\t•\tAs demais são sugestões ordenadas por confiança decrescente.\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "# 1) Cria o pipeline de Masked LM\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "\n",
    "# 2) Testa com a frase mascarada\n",
    "results = unmasker(\"Remove some parts [MASK] a sentence.\")\n",
    "\n",
    "# 3) Imprime cada sugestão\n",
    "for r in results:\n",
    "    print(f\"Token: {r['token_str']:<7}  Score: {r['score']:.4f}  Sequence: {r['sequence']}\")\n",
    "\n",
    "Token: of       0.9431  Sequence: remove some parts of a sentence.\n",
    "Token: from     0.0499  Sequence: remove some parts from a sentence.\n",
    "Token: in       0.0042  Sequence: remove some parts in a sentence.\n",
    "Token: within   0.0006  Sequence: remove some parts within a sentence.\n",
    "Token: during   0.0005  Sequence: remove some parts during a sentence.\n",
    "\n",
    "￼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da67423b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9431219100952148,\n",
       "  'token': 1997,\n",
       "  'token_str': 'of',\n",
       "  'sequence': 'remove some parts of a sentence.'},\n",
       " {'score': 0.04985547065734863,\n",
       "  'token': 2013,\n",
       "  'token_str': 'from',\n",
       "  'sequence': 'remove some parts from a sentence.'},\n",
       " {'score': 0.0042089857161045074,\n",
       "  'token': 1999,\n",
       "  'token_str': 'in',\n",
       "  'sequence': 'remove some parts in a sentence.'},\n",
       " {'score': 0.0006226670229807496,\n",
       "  'token': 2306,\n",
       "  'token_str': 'within',\n",
       "  'sequence': 'remove some parts within a sentence.'},\n",
       " {'score': 0.0005233804695308208,\n",
       "  'token': 2076,\n",
       "  'token_str': 'during',\n",
       "  'sequence': 'remove some parts during a sentence.'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "unmasker(\"Remove some parts [MASK] a sentence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868252d0",
   "metadata": {},
   "source": [
    "### Viés Algorítmico e Alucinações\n",
    "\n",
    "Note que o BERT está gerando palavras que **fazem sentido** linguístico, mas essas continuações **nem sempre correspondem à realidade**—são apenas as palavras que **maximizam a probabilidade** segundo o corpus de treino.\n",
    "\n",
    "**Exemplo**:  \n",
    "```python\n",
    "unmasker(\"Kentucky is famous for its [MASK].\")\n",
    "\n",
    "Saída (top‑5 por padrão):\n",
    "\n",
    "[{'score': 0.0757, 'token_str': 'wine',    'sequence': 'kentucky is famous for its wine.'},\n",
    " {'score': 0.0674, 'token_str': 'wines',   'sequence': 'kentucky is famous for its wines.'},\n",
    " {'score': 0.0282, 'token_str': 'beaches', 'sequence': 'kentucky is famous for its beaches.'},\n",
    " {'score': 0.0228, 'token_str': 'cuisine', 'sequence': 'kentucky is famous for its cuisine.'},\n",
    " {'score': 0.0212, 'token_str': 'horses',  'sequence': 'kentucky is famous for its horses.'}]\n",
    "\n",
    "Explicação aprofundada:\n",
    "\t•\tO modelo aprendeu a probabilidade de cada palavra aparecer após “Kentucky is famous for its” no corpus de pré‑treino, que contém muitos textos sobre wine, distillery, etc.\n",
    "\t•\tEle não “sabe” fatos verdadeiros sobre Kentucky; está apenas reproduzindo padrões estatísticos.\n",
    "\t•\tQuando o modelo produz sequências inverídicas ou implausíveis, chamamos isso de alucinação.\n",
    "\t•\tAlém disso, se o corpus tiver viés (por ex. mais textos sobre vinho), o modelo irá refletir e até amplificar esse viés — é o viés algorítmico.\n",
    "\n",
    "Em suma, BERT e outros LMs não garantem veracidade, apenas otimizam a probabilidade de ocorrência de uma sequência segundo os dados em que foram treinados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b84487",
   "metadata": {},
   "source": [
    "## Viés Algorítmico e Alucinações (continuação)\n",
    "\n",
    "Agora, substitua **“man”** por **“woman”**. Você verá que o resultado não é tão satisfatório. Esse problema **não** está na arquitetura do modelo, mas sim nos **dados** usados no pré‑treino.\n",
    "\n",
    "Podemos encontrar inúmeros exemplos de outros tipos de preconceito — existem vieses de gênero, raça e muito mais **ocultos** nos espaços de embedding do BERT.\n",
    "\n",
    "Isso é preocupante, mas lembre-se: em 2019 as pessoas ficaram impressionadas simplesmente pelo fato de o sistema gerar palavras coerentes! Hoje em dia, as saídas de LLMs geralmente passam por **mecanismos de filtragem** que identificam e bloqueiam frases potencialmente nocivas, evitando respostas ofensivas.\n",
    "\n",
    "---\n",
    "\n",
    "### Exercício\n",
    "\n",
    "Quais das seguintes afirmações são **verdadeiras** a respeito desse cenário?\n",
    "\n",
    "1. O problema de viés é inerente à arquitetura do BERT.  \n",
    "2. Os vieses refletem falhas nos dados de treinamento.  \n",
    "3. Os filtros atuais de LLMs eliminam completamente qualquer forma de preconceito.  \n",
    "4. O uso de `[MASK]` emularia o viés independentemente do modelo.  \n",
    "5. Compartilhar modelos pré‑treinados ajuda a diminuir o viés coletivo.\n",
    "\n",
    "Para cada item, justifique sua resposta com base no que discutimos sobre **dados**, **modelo** e **ferramentas de pós‑processamento**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ee5f27",
   "metadata": {},
   "source": [
    "## Respostas ao Exercício de Viés e Filtragem em LLMs\n",
    "\n",
    "1. O problema de viés é inerente à arquitetura do BERT.  \n",
    "**Resposta:** **FALSO**  \n",
    "**Justificativa:**  \n",
    "- A arquitetura Transformer (encoder‑only) do BERT **não impõe** viés de gênero, raça ou religião.  \n",
    "- O viés emerge **dos dados** utilizados no pré‑treino (textos na web, livros, artigos), que carregam estereótipos humanos.  \n",
    "- **Exemplo em código:** veja como o pipeline de preenchimento produz saídas diferentes para frases similares com “man” e “woman”:\n",
    "  ```python\n",
    "  from transformers import pipeline\n",
    "  unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "\n",
    "  # Preenchendo para “man”\n",
    "  print(unmasker(\"He is a [MASK].\")[0]['token_str'])  \n",
    "  # → 'man' pode resultar em: ['man', 'developer', 'doctor', ...]\n",
    "\n",
    "  # Preenchendo para “woman”\n",
    "  print(unmasker(\"She is a [MASK].\")[0]['token_str']) \n",
    "  # → 'woman' pode resultar em: ['woman', 'nurse', 'teacher', ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b28a8802",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "christian\n",
      "christian\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "\n",
    "# Preenchendo para “man”\n",
    "print(unmasker(\"He is a [MASK].\")[0]['token_str'])  \n",
    "# → 'man' pode resultar em: ['man', 'developer', 'doctor', ...]\n",
    "\n",
    "# Preenchendo para “woman”\n",
    "print(unmasker(\"She is a [MASK].\")[0]['token_str']) \n",
    "# → 'woman' pode resultar em: ['woman', 'nurse', 'teacher', ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8018cc",
   "metadata": {},
   "source": [
    "2. Os vieses refletem falhas nos dados de treinamento.\n",
    "\n",
    "Resposta: VERDADEIRO\n",
    "Justificativa:\n",
    "- Modelos de linguagem aprendem estatísticas de co‑ocorrência e frequência.\n",
    "- Se o dataset de pré‑treino associa “nurse” mais frequentemente com “woman” do que com “man”, o modelo reproduz esse estereótipo.\n",
    "- Para mitigar, podemos:\n",
    "- Filtrar corpora antes do pré‑treino (remover textos preconceituosos).\n",
    "- Reamostrar dados para equilibrar representações de grupos.\n",
    "- Fine‑tunar com dados contrabalanceados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4d511c",
   "metadata": {},
   "source": [
    "3. Os filtros atuais de LLMs eliminam completamente qualquer forma de preconceito.\n",
    "\n",
    "Resposta: FALSO\n",
    "Justificativa:\n",
    "- Filtros de pós‑processamento (toxicity checkers, regras de moderação) reduzem a geração de conteúdo ofensivo, mas não removem todos os vieses sutis.\n",
    "- Vieses de associação (por ex., “CEO → man”) muitas vezes passam despercebidos por sistemas de filtragem.\n",
    "- Exemplos de vieses sutis:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1a3a01",
   "metadata": {},
   "source": [
    "4. O uso de [MASK] emularia o viés independentemente do modelo.\n",
    "\n",
    "Resposta: FALSO\n",
    "Justificativa:\n",
    "- O token [MASK] é apenas uma máscara de posição — a mecânica de preenchimento depende sempre dos embeddings e do treinamento do modelo específico.\n",
    "- Se tivéssemos um modelo livre de viés, usar [MASK] não geraria estereótipos.\n",
    "- A posição [MASK] só revela o viés que já existe no espaço de embedding do modelo, não o gera."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e501891",
   "metadata": {},
   "source": [
    "5. Compartilhar modelos pré‑treinados ajuda a diminuir o viés coletivo.\n",
    "\n",
    "Resposta: VERDADEIRO (com ressalvas)\n",
    "Justificativa:\n",
    "- Quando comunidades abertas disponibilizam pesos e tokenizers, pesquisadores podem:\n",
    "- Auditar vieses (medir analogias enviesadas, disparidades de raça/gênero).\n",
    "- Propor correções (fine‑tuning, distilação consciente de vieses).\n",
    "- Exemplo de colaboração:\n",
    "- Projetos como HateCheck usam modelos públicos para avaliar e propor patches de viés.\n",
    "- Ressalva: apenas compartilhar não basta; é preciso ferramentas e boas práticas para corrigir e reavaliar continuamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232ac0f2",
   "metadata": {},
   "source": [
    "## Conclusão:\n",
    "- Arquitetura do BERT é neutra; o viés vem dos dados.\n",
    "- Filtros atenuam piora, mas não eliminam vieses profundos.\n",
    "- O token [MASK] apenas expõe o viés do modelo pré‑treinado.\n",
    "- A colaboração aberta (compartilhar modelos + auditorias) é essencial para identificar e mitigar viés em larga escala."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4239db37",
   "metadata": {},
   "source": [
    "## Tarefa 2: Next Sentence Prediction (NSP)\n",
    "\n",
    "O BERT também é pré‑treinado para a tarefa **Next Sentence Prediction**. Nela, concatenamos duas frases na entrada, separadas pelo token `[SEP]`, e usamos o vetor do token `[CLS]` (denotado como \\(C\\)) para classificar se a segunda frase realmente segue a primeira.\n",
    "\n",
    "A ideia é:\n",
    "\n",
    "1. **Exemplo positivo**  \n",
    "   ```text\n",
    "   [CLS] Here I am [SEP] rock you like a hurricane\n",
    "\n",
    "→ o rótulo (“IsNext”) deve ser 1.\n",
    "\t2.\tExemplo negativo\n",
    "\n",
    "[CLS] Here I am [SEP] rock your body\n",
    "\n",
    "→ o rótulo (“IsNext”) deve ser 0.\n",
    "\n",
    "Após o pré‑treino, o vetor (C) captura a relação entre as duas sentenças, permitindo usá‑lo como feature em tarefas de classificação de sequência.\n",
    "\n",
    "⸻\n",
    "\n",
    "Usando a Hugging Face para NSP\n",
    "\n",
    "Podemos usar a classe AutoModelForNextSentencePrediction e o AutoTokenizer da biblioteca Transformers para executar NSP diretamente:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff81d73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positivo (esperado IsNext ~1): {'IsNext': 0.9918953776359558, 'NotNext': 0.008104616776108742}\n",
      "Negativo (esperado NotNext ~1): {'IsNext': 0.04612401872873306, 'NotNext': 0.9538760185241699}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForNextSentencePrediction\n",
    "import torch\n",
    "\n",
    "# 1) Carregue o tokenizer e o modelo (inclui a cabeça NSP)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model     = AutoModelForNextSentencePrediction.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Função auxiliar para avaliar NSP\n",
    "def is_next_sentence(sent_a, sent_b):\n",
    "    # Tokeniza as duas sentenças, inserindo [CLS] ... [SEP] ... [SEP]\n",
    "    inputs = tokenizer(sent_a, sent_b, return_tensors=\"pt\")\n",
    "    # Obtém os logits e converte em probabilidades\n",
    "    logits = model(**inputs).logits  # shape [1, 2]: [IsNext, NotNext]\n",
    "    probs  = torch.softmax(logits, dim=1).squeeze().tolist()\n",
    "    return {\"IsNext\": probs[0], \"NotNext\": probs[1]}\n",
    "\n",
    "# 2) Teste com um par de sentenças que realmente seguem\n",
    "result_pos = is_next_sentence(\n",
    "    \"Here I am\", \n",
    "    \"rock you like a hurricane\"\n",
    ")\n",
    "print(\"Positivo (esperado IsNext ~1):\", result_pos)\n",
    "\n",
    "# 3) Teste com um par de sentenças que NÃO seguem\n",
    "result_neg = is_next_sentence(\n",
    "    \"Here I am\", \n",
    "    \"rock your body\"\n",
    ")\n",
    "print(\"Negativo (esperado NotNext ~1):\", result_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5444568",
   "metadata": {},
   "source": [
    "\n",
    "Explicação dos passos:\n",
    " 1. Tokenização:\n",
    "- tokenizer(sent_a, sent_b, return_tensors=\"pt\") insere automaticamente os tokens [CLS], [SEP] e segment embeddings para cada sentença.\n",
    " 2. Inferência:\n",
    "- model(**inputs).logits retorna um tensor de forma [batch_size, 2] contendo os logits para as classes IsNext (índice 0) e NotNext (índice 1).\n",
    " 3. Probabilidades:\n",
    "- Aplicamos softmax sobre os logits para obter probabilidades, e assim podemos verificar qual das duas classes o modelo considera mais provável.\n",
    "\n",
    "Dessa forma, utilizamos o pré‑treino NSP do BERT para decidir se uma frase segue logicamente outra, extraindo do token [CLS] um vetor de classificação que representa a relação entre as sentenças.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f388a3f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5db448f2",
   "metadata": {},
   "source": [
    "### Carregando o Tokenizer e o Modelo BERT\n",
    "\n",
    "Nesta etapa, vamos carregar o tokenizer e o modelo pré‑treinado `bert-base-uncased` da Hugging Face:\n",
    "\n",
    "- **Tokenizer**: converte texto em IDs de tokens (WordPieces) e gera tensores prontos para o modelo.  \n",
    "- **Modelo**: `BertModel` retorna, para cada token, um vetor contextualizado de dimensão igual ao hidden size (geralmente 768 em `bert-base-uncased`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79f2a97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# 1) Carrega o tokenizer e o modelo pré‑treinado\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model     = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 2) Prepara um texto qualquer\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "\n",
    "# 3) Tokeniza e retorna tensores PyTorch\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "# 4) Executa o modelo (torch.no_grad() pode ser usado em inferência)\n",
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550c78e9",
   "metadata": {},
   "source": [
    "Explicação detalhada:\n",
    "- encoded_input é um dicionário com chaves como input_ids e attention_mask, ambos tensores de shape [batch_size, seq_len].\n",
    "- output é um objeto do tipo BaseModelOutputWithPoolingAndCrossAttentions contendo:\n",
    "- last_hidden_state: tensor [batch_size, seq_len, hidden_size] com os embeddings de cada token.\n",
    "- pooler_output: tensor [batch_size, hidden_size] que é uma projeção do embedding do token [CLS]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed3dd46",
   "metadata": {},
   "source": [
    "## Extraindo o Embedding do Token [CLS]\n",
    "\n",
    "Para usar o vetor que representa toda a sequência (útil em classificação), basta capturar a primeira posição de last_hidden_state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8f11f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'output.last_hidden_state' tem shape [1, seq_len, hidden_size]\n",
    "# [0,0,:] significa: batch index 0, token index 0 ([CLS]), todos os hidden units\n",
    "output_cls = output.last_hidden_state[0, 0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f7976a",
   "metadata": {},
   "source": [
    "Dica de atalho:\n",
    "- Você também pode usar diretamente output.pooler_output, que já é o embedding resultante do [CLS] passado por uma camada adicional de ativação."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e25fb13",
   "metadata": {},
   "source": [
    "### Exploração completa da geração de embeddings no BERT\n",
    "\n",
    "O BERT gera embeddings em várias etapas bem definidas. Abaixo, detalhamos cada componente e mostramos como o modelo os combina internamente.\n",
    "\n",
    "1. **Tokenização**  \n",
    "   - O texto é dividido em *WordPieces* e transformado em IDs de tokens (`input_ids`).  \n",
    "   - São gerados também:\n",
    "     - `token_type_ids` (segment embeddings): indicam se cada token pertence à sentença A ou B.  \n",
    "     - `attention_mask`: máscara para ignorar padding.  \n",
    "\n",
    "2. **Camada de Embedding**  \n",
    "   - **Word embeddings**:  \n",
    "     ```python\n",
    "     word_emb = model.embeddings.word_embeddings(input_ids)\n",
    "     ```\n",
    "   - **Position embeddings**:  \n",
    "     ```python\n",
    "     # Cria position_ids = [[0,1,2,...], [0,1,2,...], ...]\n",
    "     position_ids = torch.arange(input_ids.size(1)).unsqueeze(0).expand(input_ids.size())\n",
    "     pos_emb = model.embeddings.position_embeddings(position_ids)\n",
    "     ```\n",
    "   - **Token type embeddings**:  \n",
    "     ```python\n",
    "     token_type_emb = model.embeddings.token_type_embeddings(token_type_ids)\n",
    "     ```\n",
    "   - **Soma dos três embeddings**:\n",
    "     ```python\n",
    "     inputs_embeds = word_emb + pos_emb + token_type_emb\n",
    "     ```\n",
    "   - **LayerNorm e Dropout**:\n",
    "     ```python\n",
    "     inputs_embeds = model.embeddings.LayerNorm(inputs_embeds)\n",
    "     inputs_embeds = model.embeddings.dropout(inputs_embeds)\n",
    "     ```\n",
    "\n",
    "3. **Encoder (Transformer Layers)**  \n",
    "   - Em `bert-base-uncased`, são 12 camadas de encoder, cada uma com:\n",
    "     - *Multi‑Head Self‑Attention* (8 cabeças)  \n",
    "     - *Feed-Forward* (hidden_size→4×hidden_size→hidden_size)  \n",
    "     - *Residual + LayerNorm* em cada subcamada  \n",
    "   - Código aproximado:\n",
    "     ```python\n",
    "     encoder_outputs = model.encoder(\n",
    "         inputs_embeds,\n",
    "         attention_mask=attention_mask\n",
    "     )\n",
    "     last_hidden_state = encoder_outputs.last_hidden_state\n",
    "     # shape: [batch_size, seq_len, hidden_size]\n",
    "     ```\n",
    "\n",
    "4. **Pooler (Embedding de sentença)**  \n",
    "   - Seleciona o vetor do token `[CLS]` (posição 0):  \n",
    "     ```python\n",
    "     cls_token_emb = last_hidden_state[:, 0, :]  # [batch_size, hidden_size]\n",
    "     ```\n",
    "   - Aplica uma camada linear + `tanh`:  \n",
    "     ```python\n",
    "     pooler_output = model.pooler(cls_token_emb)\n",
    "     # shape: [batch_size, hidden_size]\n",
    "     ```\n",
    "\n",
    "5. **Saídas**  \n",
    "   - `last_hidden_state` → embeddings contextuais para **cada** token.  \n",
    "   - `pooler_output`     → embedding de **toda** a sequência (útil em classificação downstream).\n",
    "\n",
    "---\n",
    "\n",
    "Abaixo, o código completo mostrando cada etapa:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# 1) Carrega tokenizer e modelo\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model     = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 2) Exemplo de batch de sentenças\n",
    "texts = [\"Hello world!\", \"How are you doing?\"]\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "input_ids      = inputs['input_ids']       # [batch_size, seq_len]\n",
    "token_type_ids = inputs.get('token_type_ids', torch.zeros_like(input_ids))\n",
    "attention_mask = inputs['attention_mask']\n",
    "\n",
    "# 3) WordPiece embeddings\n",
    "word_emb = model.embeddings.word_embeddings(input_ids)\n",
    "\n",
    "# 4) Position embeddings\n",
    "position_ids = torch.arange(input_ids.size(1)).unsqueeze(0).expand(input_ids.size())\n",
    "pos_emb = model.embeddings.position_embeddings(position_ids)\n",
    "\n",
    "# 5) Token type embeddings\n",
    "token_type_emb = model.embeddings.token_type_embeddings(token_type_ids)\n",
    "\n",
    "# 6) Soma + LayerNorm + Dropout\n",
    "inputs_embeds = word_emb + pos_emb + token_type_emb\n",
    "inputs_embeds = model.embeddings.LayerNorm(inputs_embeds)\n",
    "inputs_embeds = model.embeddings.dropout(inputs_embeds)\n",
    "\n",
    "# 7) Passa pelo encoder (todas as camadas Transformer)\n",
    "encoder_outputs     = model.encoder(inputs_embeds, attention_mask=attention_mask)\n",
    "last_hidden_state   = encoder_outputs.last_hidden_state  # [batch, seq_len, hidden_size]\n",
    "\n",
    "# 8) Pooler: embedding de sequência via [CLS]\n",
    "cls_token_emb  = last_hidden_state[:, 0, :]        # [batch, hidden_size]\n",
    "pooler_output  = model.pooler(cls_token_emb)       # [batch, hidden_size]\n",
    "\n",
    "# 9) Verifica shapes\n",
    "print(\"last_hidden_state shape:\", last_hidden_state.shape)\n",
    "print(\"pooler_output    shape:\", pooler_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed5780f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_hidden_state shape: torch.Size([2, 7, 768])\n",
      "pooler_output    shape: torch.Size([2, 768])\n"
     ]
    }
   ],
   "source": [
    "# 1) Carrega tokenizer e modelo\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model     = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 2) Tokeniza\n",
    "texts = [\"Hello world!\", \"How are you doing?\"]\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# 3) Converte attention_mask para float ou bool\n",
    "inputs['attention_mask'] = inputs['attention_mask'].to(dtype=torch.float)  # ou .bool()\n",
    "\n",
    "# 4) Monta embeddings de entrada manualmente (opcional)\n",
    "word_emb       = model.embeddings.word_embeddings(inputs['input_ids'])\n",
    "position_ids   = torch.arange(inputs['input_ids'].size(1)).unsqueeze(0).expand(inputs['input_ids'].size())\n",
    "pos_emb        = model.embeddings.position_embeddings(position_ids)\n",
    "token_type_emb = model.embeddings.token_type_embeddings(inputs.get('token_type_ids', torch.zeros_like(inputs['input_ids'])))\n",
    "\n",
    "inputs_embeds  = word_emb + pos_emb + token_type_emb\n",
    "inputs_embeds  = model.embeddings.LayerNorm(inputs_embeds)\n",
    "inputs_embeds  = model.embeddings.dropout(inputs_embeds)\n",
    "\n",
    "# 5) Chama o modelo completo — ele faz a conversão interna da máscara\n",
    "outputs = model(\n",
    "    inputs_embeds=inputs_embeds,\n",
    "    attention_mask=inputs['attention_mask']\n",
    ")\n",
    "\n",
    "last_hidden_state = outputs.last_hidden_state  # [batch_size, seq_len, hidden_size]\n",
    "pooler_output     = outputs.pooler_output      # [batch_size, hidden_size]\n",
    "\n",
    "print(\"last_hidden_state shape:\", last_hidden_state.shape)\n",
    "print(\"pooler_output    shape:\", pooler_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c87331",
   "metadata": {},
   "source": [
    "## Exercício: Classificação usando embeddings do BERT\n",
    "\n",
    "Em vez de usar TF‑IDF + Regressão Logística, podemos extrair **embeddings contextuais** de cada sinopse de filme usando um BERT pré‑treinado e então treinar um classificador sobre esses vetores:\n",
    "\n",
    "1. **Carregar dados e dividir em treino/teste** — como antes.  \n",
    "2. **Tokenizar e gerar embeddings**:\n",
    "   - Usar `AutoTokenizer` + `AutoModel` para obter `pooler_output` (ou o embedding do token `[CLS]`) para cada texto.  \n",
    "   - Agregar num array de shape `(n_samples, hidden_size)`.\n",
    "\n",
    "3. **Treinar um classificador** (por exemplo, `LogisticRegression`) sobre esses embeddings.  \n",
    "4. **Avaliar** usando `classification_report`.\n",
    "\n",
    "Esse fluxo mostra o poder de embeddings pré‑treinados: sem engenharia de features, aproveitamos representações ricas aprendidas em corpora massivos.\n",
    "\n",
    "---\n",
    "\n",
    "### Passos gerais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a0465b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84167d7",
   "metadata": {},
   "source": [
    "1.\tCarregar e dividir os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a0df10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/tiagoft/NLP/main/wiki_movie_plots_drama_comedy.csv')\n",
    "df = df.sample(1000, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['Plot'], df['Genre'], test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62080bf",
   "metadata": {},
   "source": [
    "2.\tConfigurar BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b81410d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model     = AutoModel.from_pretrained('bert-base-uncased')\n",
    "model.eval()  # modo inferência"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2975d5",
   "metadata": {},
   "source": [
    "3.\tFunção para extrair embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46e7cb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(texts, batch_size=16, max_len=128):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size].tolist()\n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        # pooler_output: [batch_size, hidden_size]\n",
    "        embs = outputs.pooler_output.cpu().numpy()\n",
    "        embeddings.append(embs)\n",
    "    return np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e058959",
   "metadata": {},
   "source": [
    "4.\tGerar embeddings para treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "104377f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_emb = get_embeddings(X_train, batch_size=32)\n",
    "X_test_emb  = get_embeddings(X_test,  batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be33fcd6",
   "metadata": {},
   "source": [
    "5.\tTreinar e avaliar o classificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9649f72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      comedy       0.63      0.59      0.61        87\n",
      "       drama       0.70      0.73      0.72       113\n",
      "\n",
      "    accuracy                           0.67       200\n",
      "   macro avg       0.66      0.66      0.66       200\n",
      "weighted avg       0.67      0.67      0.67       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train_emb, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_emb)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e14efa3",
   "metadata": {},
   "source": [
    "Dessa forma, você compara diretamente o desempenho de um classificador baseado em embeddings contextuais do BERT com o pipeline clássico de TF‑IDF. Isso costuma melhorar a acurácia em tarefas de texto, sobretudo quando há ambiguidade ou relações semânticas complexas nos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bed406",
   "metadata": {},
   "source": [
    "### Classificação com Embeddings do BERT (Solução do Professor)\n",
    "\n",
    "A seguir, apresentamos um fluxo completo para:\n",
    "\n",
    "1. **Carregar e amostrar os dados**  \n",
    "2. **Pré‑processar textos** e **extrair embeddings** do token `[CLS]` usando o BERT  \n",
    "3. **Salvar** e **recarregar** os embeddings em disco  \n",
    "4. **Dividir** em treino/teste  \n",
    "5. **Treinar** um classificador (`LogisticRegression`) sobre esses vetores  \n",
    "6. **Avaliar** o desempenho com `classification_report`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed7c78c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computando embeddings: 100%|██████████| 1000/1000 [02:14<00:00,  7.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      comedy       0.68      0.76      0.72        85\n",
      "       drama       0.81      0.74      0.77       115\n",
      "\n",
      "    accuracy                           0.75       200\n",
      "   macro avg       0.75      0.75      0.75       200\n",
      "weighted avg       0.76      0.75      0.75       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Passo 0: Carregar e amostrar os dados\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\n",
    "    'https://raw.githubusercontent.com/tiagoft/NLP/main/wiki_movie_plots_drama_comedy.csv'\n",
    ").sample(1000, random_state=42)\n",
    "\n",
    "X = df['Plot']    # sinopses de filmes\n",
    "y = df['Genre']   # drama ou comedy\n",
    "\n",
    "# Passo 1: Pré‑processamento e extração de embeddings\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Carrega tokenizer e modelo BERT pré‑treinado\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model     = BertModel.from_pretrained('bert-base-uncased')\n",
    "model.eval()  # modo inferência (desativa dropout)\n",
    "\n",
    "def get_embeddings(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Retorna o embedding do token [CLS] para a string `text`.\n",
    "    \"\"\"\n",
    "    # Tokeniza, adicionando padding/truncation até max_length=512\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    # Inhibe cálculo de gradiente\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Pega o vetor correspondente ao [CLS] (posição 0)\n",
    "    cls_embedding = outputs.last_hidden_state[0, 0, :]\n",
    "    return cls_embedding\n",
    "\n",
    "# Gera embeddings para todas as sinopses\n",
    "embeddings = []\n",
    "for plot in tqdm(X, desc='Computando embeddings'):\n",
    "    emb = get_embeddings(plot, model, tokenizer)\n",
    "    embeddings.append(emb.numpy())\n",
    "\n",
    "# Converte em array NumPy e salva em disco\n",
    "embeddings = np.stack(embeddings)  # shape: [1000, hidden_size]\n",
    "np.save('bert_embeddings.npy', embeddings)\n",
    "\n",
    "# Passo 2: Recarrega embeddings (caso precise acelerar iterações)\n",
    "embeddings = np.load('bert_embeddings.npy')\n",
    "\n",
    "# Passo 3: Divide em treino e teste\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    embeddings, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Passo 4: Treina o classificador\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Passo 5: Faz previsões e imprime relatório\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd95b5f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
