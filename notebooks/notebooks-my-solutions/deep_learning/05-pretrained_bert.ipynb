{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f405861c",
   "metadata": {},
   "source": [
    "# BERT\n",
    "\n",
    "**OBJETIVO:** Ao final desta aula, voc√™ ser√° capaz de **usar um BERT pr√©‚Äëtreinado** para:  \n",
    "1. Gerar **sugest√µes** (por exemplo, preencher lacunas de texto ou completar senten√ßas mascaradas).  \n",
    "2. Extrair **embeddings contextuais** para tarefas de classifica√ß√£o.\n",
    "\n",
    "> **Explica√ß√£o aprofundada:**  \n",
    "> - **BERT** (Bidirectional Encoder Representations from Transformers) √© um modelo de linguagem baseado em Transformer, pr√©‚Äëtreinado em grandes corpora usando:\n",
    ">   - **Masked Language Modeling** (MLM): algumas palavras s√£o ocultadas e o modelo aprende a prever as palavras mascaradas.  \n",
    ">   - **Next Sentence Prediction** (NSP): o modelo aprende a relacionar senten√ßas em sequ√™ncia.\n",
    "> - Ao usar um BERT pr√©‚Äëtreinado:\n",
    ">   1. **Tokenizer**: converte texto em IDs de sub‚Äëpalavras (WordPieces).  \n",
    ">   2. **Modelo**: responde tarefas como ‚Äúfill‚Äëmask‚Äù ou retorna um tensor de embeddings para cada token, que podemos agregar (p.ex., via m√©dia ou usando o token `[CLS]`) para alimentar um classificador.\n",
    "> - **Sugest√µes** (a): podemos usar a fun√ß√£o de preenchimento de m√°scara para completar palavras ou frases de forma natural.  \n",
    "> - **Embeddings** (b): extra√≠mos vetores que capturam o contexto de cada token/senten√ßa e servem como **features** poderosas em modelos de classifica√ß√£o downstream."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2582c6fd",
   "metadata": {},
   "source": [
    "## O que √© o BERT?\n",
    "\n",
    "Ap√≥s o surgimento do [Transformer](https://arxiv.org/abs/1706.03762) ‚Äî que revolucionou o processamento de sequ√™ncias ao introduzir o mecanismo de **aten√ß√£o** em m√∫ltiplas cabe√ßas, sem recorrer a arquiteturas recorrentes ‚Äî vieram diversos avan√ßos. Um deles foi o [GPT](https://paperswithcode.com/paper/improving-language-understanding-by), que adota **apenas o decodificador** do Transformer para realizar a tarefa de **previs√£o do pr√≥ximo token**. Nesse esquema, o GPT usa m√°scaras unidirecionais na aten√ß√£o (cada posi√ß√£o s√≥ ‚Äúv√™‚Äù o que vem **antes**), evitando previs√µes triviais e aprendendo a gerar continuations de texto coerentes. O resultado √© um espa√ßo de embeddings em que tokens plaus√≠veis para a sequ√™ncia aparecem com maior probabilidade.\n",
    "\n",
    "O time do Google prop√¥s outra abordagem inovadora: em vez de decodificador, usar **apenas o encoder** do Transformer e treinar o modelo para duas tarefas simult√¢neas:\n",
    "\n",
    "1. **Masked Language Modeling (MLM)**  \n",
    "   - Algumas palavras (ou subpalavras) s√£o **mascaradas** (substitu√≠das por um token especial `[MASK]`) durante o pr√©‚Äëtreino.  \n",
    "   - O modelo deve **predizer a palavra mascarada** usando informa√ß√µes de **todo** o contexto ‚Äî tanto o que vem antes **quanto** o que vem depois.  \n",
    "   - Exemplo: em ‚ÄúLuke, I am your `[MASK]`‚Äù, o BERT observa ‚ÄúLuke, I am your ____‚Äù e usa os dois lados para preencher o espa√ßo em branco.\n",
    "\n",
    "2. **Next Sentence Prediction (NSP)**  \n",
    "   - Dada uma dupla de senten√ßas \\((A, B)\\), o modelo aprende a classificar se **B realmente segue A** no corpus original.  \n",
    "   - Isso refor√ßa a capacidade de capturar rela√ß√µes de sequ√™ncia entre frases, √∫til em tarefas como pergunta‚Äëresposta e infer√™ncia textual.\n",
    "\n",
    "Com isso, nasceu o **BERT** (Bidirectional Encoder Representations from Transformers).  \n",
    "> **Por que ‚Äúbidirecional‚Äù?**  \n",
    "> Porque, ao contr√°rio de modelos unidirecionais (como o GPT), o BERT aproveita **ambos os lados** do contexto em cada posi√ß√£o, enriquecendo a representa√ß√£o de cada token.\n",
    "\n",
    "Ao final do pr√©‚Äëtreino, temos um modelo capaz de:\n",
    "- **Gerar sugest√µes** precisas para completar trechos de texto.  \n",
    "- **Extrair embeddings** contextuais (por exemplo, via o vetor do token `[CLS]`) para alimentar classificadores em tarefas downstream, como classifica√ß√£o de sentimentos, perguntas e respostas, detec√ß√£o de entidades etc.  \n",
    "\n",
    "Esses recursos fazem do BERT a base de in√∫meras aplica√ß√µes de NLP de √∫ltima gera√ß√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7da52fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 12.2.1 (20241206.2353)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"625pt\" height=\"344pt\"\n",
       " viewBox=\"0.00 0.00 625.00 344.47\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 340.47)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-340.47 621,-340.47 621,4 -4,4\"/>\n",
       "<g id=\"clust1\" class=\"cluster\">\n",
       "<title>cluster_input</title>\n",
       "<path fill=\"lightgrey\" stroke=\"lightgrey\" d=\"M20,-157.98C20,-157.98 597,-157.98 597,-157.98 603,-157.98 609,-163.98 609,-169.98 609,-169.98 609,-316.47 609,-316.47 609,-322.47 603,-328.47 597,-328.47 597,-328.47 20,-328.47 20,-328.47 14,-328.47 8,-322.47 8,-316.47 8,-316.47 8,-169.98 8,-169.98 8,-163.98 14,-157.98 20,-157.98\"/>\n",
       "<text text-anchor=\"middle\" x=\"308.5\" y=\"-311.17\" font-family=\"Times,serif\" font-size=\"14.00\">Input</text>\n",
       "</g>\n",
       "<!-- T -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>T</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"98\" cy=\"-266.97\" rx=\"82.06\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"98\" y=\"-261.92\" font-family=\"Times,serif\" font-size=\"14.00\">Token embeddings</text>\n",
       "</g>\n",
       "<!-- ADD -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>ADD</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"287\" cy=\"-183.98\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"287\" y=\"-178.93\" font-family=\"Times,serif\" font-size=\"14.00\">+</text>\n",
       "</g>\n",
       "<!-- T&#45;&gt;ADD -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>T&#45;&gt;ADD</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M133.99,-250.55C168.79,-235.64 221.02,-213.26 254.56,-198.88\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"255.54,-202.27 263.35,-195.12 252.78,-195.84 255.54,-202.27\"/>\n",
       "</g>\n",
       "<!-- P -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>P</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"287\" cy=\"-266.97\" rx=\"88.71\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"287\" y=\"-261.92\" font-family=\"Times,serif\" font-size=\"14.00\">Position embeddings</text>\n",
       "</g>\n",
       "<!-- P&#45;&gt;ADD -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>P&#45;&gt;ADD</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M287,-248.8C287,-238.67 287,-225.58 287,-213.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"290.5,-213.93 287,-203.93 283.5,-213.93 290.5,-213.93\"/>\n",
       "</g>\n",
       "<!-- S -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>S</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"497\" cy=\"-266.97\" rx=\"103.59\" ry=\"28.99\"/>\n",
       "<text text-anchor=\"middle\" x=\"497\" y=\"-270.17\" font-family=\"Times,serif\" font-size=\"14.00\">Segment embeddings</text>\n",
       "<text text-anchor=\"middle\" x=\"497\" y=\"-253.67\" font-family=\"Times,serif\" font-size=\"14.00\">(indica senten√ßa 1 ou 2)</text>\n",
       "</g>\n",
       "<!-- S&#45;&gt;ADD -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>S&#45;&gt;ADD</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M437.1,-242.87C399.26,-228.28 351.76,-209.96 320.45,-197.88\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"322.09,-194.77 311.5,-194.43 319.57,-201.3 322.09,-194.77\"/>\n",
       "</g>\n",
       "<!-- SEQ -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>SEQ</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"287\" cy=\"-111.98\" rx=\"73.36\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"287\" y=\"-106.93\" font-family=\"Times,serif\" font-size=\"14.00\">Sequence Model</text>\n",
       "</g>\n",
       "<!-- ADD&#45;&gt;SEQ -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>ADD&#45;&gt;SEQ</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M287,-165.68C287,-158.39 287,-149.71 287,-141.52\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"290.5,-141.6 287,-131.6 283.5,-141.6 290.5,-141.6\"/>\n",
       "</g>\n",
       "<!-- RES -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>RES</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"287\" cy=\"-28.99\" rx=\"120.03\" ry=\"28.99\"/>\n",
       "<text text-anchor=\"middle\" x=\"287\" y=\"-32.19\" font-family=\"Times,serif\" font-size=\"14.00\">Resultado:</text>\n",
       "<text text-anchor=\"middle\" x=\"287\" y=\"-15.69\" font-family=\"Times,serif\" font-size=\"14.00\">1 vetor por token de entrada</text>\n",
       "</g>\n",
       "<!-- SEQ&#45;&gt;RES -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>SEQ&#45;&gt;RES</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M287,-93.81C287,-86.72 287,-78.19 287,-69.7\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"290.5,-69.81 287,-59.81 283.5,-69.81 290.5,-69.81\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x10440b820>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "dot = Digraph(format='png')\n",
    "# Agrupamento ‚ÄúInput‚Äù\n",
    "with dot.subgraph(name='cluster_input') as c:\n",
    "    c.attr(label='Input', style='rounded,filled', color='lightgrey')\n",
    "    c.node('T', 'Token embeddings')\n",
    "    c.node('P', 'Position embeddings')\n",
    "    c.node('S', 'Segment embeddings\\n(indica senten√ßa 1 ou 2)')\n",
    "    c.node('ADD', '+')\n",
    "    # Conex√µes dentro do subgraph\n",
    "    c.edge('T', 'ADD')\n",
    "    c.edge('P', 'ADD')\n",
    "    c.edge('S', 'ADD')\n",
    "\n",
    "# Fluxo principal\n",
    "dot.node('SEQ', 'Sequence Model')\n",
    "dot.node('RES', 'Resultado:\\n1 vetor por token de entrada')\n",
    "\n",
    "dot.edge('ADD', 'SEQ')\n",
    "dot.edge('SEQ', 'RES')\n",
    "\n",
    "# Exibir no Jupyter\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f483ee",
   "metadata": {},
   "source": [
    "Continuando:\n",
    "\n",
    "O nome **BERT** vem de **Bidirectional Encoder Representations from Transformers** e foi apresentado neste [artigo de 2019](https://arxiv.org/pdf/1810.04805). A grande inova√ß√£o do BERT, al√©m da arquitetura Transformer ‚Äúsomente encoder‚Äù, √© o **pr√©‚Äëtreino multitarefa**, em que o modelo aprende simultaneamente:\n",
    "\n",
    "1. **Masked Language Modeling (MLM)** ‚Äì prever palavras mascaradas usando contexto bidirecional.  \n",
    "2. **Next Sentence Prediction (NSP)** ‚Äì classificar se duas senten√ßas realmente aparecem em sequ√™ncia.\n",
    "\n",
    "N√£o vamos treinar o BERT do zero em aula (√© muito pesado!), mas sim **utilizar um modelo pr√©‚Äëtreinado** para tarefas downstream. Faremos isso usando a implementa√ß√£o oficial do BERT pela Hugging Face:  \n",
    "> **bert-base-uncased**  \n",
    "> üìö Documenta√ß√£o completa: https://huggingface.co/google-bert/bert-base-uncased\n",
    "\n",
    "---\n",
    "\n",
    "## Tarefa 1: Masked Language Model\n",
    "\n",
    "A primeira tarefa no pr√©‚Äëtreino do BERT foi o **Masked Language Model**, inspirada nos testes de ‚Äúcloze‚Äù em lingu√≠stica. A ideia √©:\n",
    "\n",
    "- **Remover** (ou substituir por `[MASK]`) uma ou mais palavras numa frase.  \n",
    "- **Pedir** ao modelo que **preveja** qual √© a palavra que falta, usando **todo** o contexto (antes **e** depois de `[MASK]`).  \n",
    "\n",
    "> **Exemplo de cloze**:  \n",
    "> > ‚ÄúLuke, I am your¬†`[MASK]`.‚Äù  \n",
    ">  \n",
    "> O BERT v√™ `\"Luke, I am your __ .\"` e utiliza tanto as palavras anteriores (**Luke, I am your**) quanto as posteriores (neste caso, o ponto final) para descobrir que a resposta l√≥gica √© **‚Äúfather‚Äù**.\n",
    "\n",
    "**Pr√≥ximos passos**:  \n",
    "- Vamos carregar o tokenizer e o modelo pr√©‚Äëtreinado do Hugging Face.  \n",
    "- Construir um pipeline de preenchimento de m√°scara (`fill-mask`) para experimentar com frases mascaradas.  \n",
    "- Discutir como extrair as probabilidades de cada palavra candidata e interpretar os resultados.\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "# 1) Cria um pipeline de Masked LM usando o BERT pr√©‚Äëtreinado\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"google/bert-base-uncased\",\n",
    "    tokenizer=\"google/bert-base-uncased\"\n",
    ")\n",
    "\n",
    "# 2) Testa com uma frase mascarada\n",
    "resultado = fill_mask(\"Luke, I am your [MASK].\")\n",
    "for r in resultado:\n",
    "    print(f\"{r['token_str']}: probabilidade {r['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413b225e",
   "metadata": {},
   "source": [
    "Explica√ß√£o aprofundada:\n",
    "- O pipeline de fill-mask j√°:\n",
    "\t1.\tTokeniza o texto, insertando [MASK].\n",
    "\t2.\tPassa pelo modelo BERT para obter logits em cada posi√ß√£o.\n",
    "\t3.\tAplica Softmax apenas sobre o vocabul√°rio na posi√ß√£o mascarada.\n",
    "\t4.\tRetorna as top‚Äëk palavras mais prov√°veis para substituir [MASK], junto com suas probabilidades.\n",
    "- Isso ilustra como o BERT usa o contexto bidirecional para preencher lacunas de forma muito mais precisa que modelos unidirecionais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c3e07ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "father: 0.2734\n",
      "brother: 0.1040\n",
      "mother: 0.1030\n",
      "friend: 0.0900\n",
      "son: 0.0722\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, pipeline\n",
    "\n",
    "# 1) Carregue manualmente o tokenizer e o modelo com local_files_only=True\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    local_files_only=True\n",
    ")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "# 2) Crie o pipeline passando os objetos j√° carregados\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# 3) Use normalmente\n",
    "resultados = fill_mask(\"Luke, I am your [MASK].\")\n",
    "for r in resultados:\n",
    "    print(f\"{r['token_str']}: {r['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c0dce8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 12.2.1 (20241206.2353)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"199pt\" height=\"582pt\"\n",
       " viewBox=\"0.00 0.00 198.75 582.31\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 578.31)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-578.31 194.75,-578.31 194.75,4 -4,4\"/>\n",
       "<g id=\"clust1\" class=\"cluster\">\n",
       "<title>cluster_inputs</title>\n",
       "<path fill=\"lightgrey\" stroke=\"lightgrey\" d=\"M53.38,-351.16C53.38,-351.16 137.38,-351.16 137.38,-351.16 143.38,-351.16 149.38,-357.16 149.38,-363.16 149.38,-363.16 149.38,-554.31 149.38,-554.31 149.38,-560.31 143.38,-566.31 137.38,-566.31 137.38,-566.31 53.38,-566.31 53.38,-566.31 47.38,-566.31 41.38,-560.31 41.38,-554.31 41.38,-554.31 41.38,-363.16 41.38,-363.16 41.38,-357.16 47.38,-351.16 53.38,-351.16\"/>\n",
       "<text text-anchor=\"middle\" x=\"95.38\" y=\"-549.01\" font-family=\"Times,serif\" font-size=\"14.00\">Inputs</text>\n",
       "</g>\n",
       "<g id=\"clust2\" class=\"cluster\">\n",
       "<title>cluster_outputs</title>\n",
       "<path fill=\"lightgrey\" stroke=\"lightgrey\" d=\"M72.38,-64C72.38,-64 118.38,-64 118.38,-64 124.38,-64 130.38,-70 130.38,-76 130.38,-76 130.38,-267.16 130.38,-267.16 130.38,-273.16 124.38,-279.16 118.38,-279.16 118.38,-279.16 72.38,-279.16 72.38,-279.16 66.38,-279.16 60.38,-273.16 60.38,-267.16 60.38,-267.16 60.38,-76 60.38,-76 60.38,-70 66.38,-64 72.38,-64\"/>\n",
       "<text text-anchor=\"middle\" x=\"95.38\" y=\"-261.86\" font-family=\"Times,serif\" font-size=\"14.00\">Outputs</text>\n",
       "</g>\n",
       "<!-- INPUT -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>INPUT</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"95.38\" cy=\"-446.48\" rx=\"45.79\" ry=\"87.33\"/>\n",
       "<text text-anchor=\"middle\" x=\"95.38\" y=\"-490.93\" font-family=\"Times,serif\" font-size=\"14.00\">[CLS]</text>\n",
       "<text text-anchor=\"middle\" x=\"95.38\" y=\"-474.43\" font-family=\"Times,serif\" font-size=\"14.00\">remove</text>\n",
       "<text text-anchor=\"middle\" x=\"95.38\" y=\"-457.93\" font-family=\"Times,serif\" font-size=\"14.00\">some</text>\n",
       "<text text-anchor=\"middle\" x=\"95.38\" y=\"-441.43\" font-family=\"Times,serif\" font-size=\"14.00\">parts</text>\n",
       "<text text-anchor=\"middle\" x=\"95.38\" y=\"-424.93\" font-family=\"Times,serif\" font-size=\"14.00\">[MASK]</text>\n",
       "<text text-anchor=\"middle\" x=\"95.38\" y=\"-408.43\" font-family=\"Times,serif\" font-size=\"14.00\">a</text>\n",
       "<text text-anchor=\"middle\" x=\"95.38\" y=\"-391.93\" font-family=\"Times,serif\" font-size=\"14.00\">sentence</text>\n",
       "</g>\n",
       "<!-- BERT -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>BERT</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"95.38\" cy=\"-305.16\" rx=\"34.46\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"95.38\" y=\"-300.11\" font-family=\"Times,serif\" font-size=\"14.00\">BERT</text>\n",
       "</g>\n",
       "<!-- INPUT&#45;&gt;BERT -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>INPUT&#45;&gt;BERT</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M95.38,-358.83C95.38,-350.18 95.38,-341.9 95.38,-334.57\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"98.88,-334.76 95.38,-324.76 91.88,-334.76 98.88,-334.76\"/>\n",
       "</g>\n",
       "<!-- OUTPUT -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>OUTPUT</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"95.38\" cy=\"-159.33\" rx=\"27\" ry=\"87.33\"/>\n",
       "<text text-anchor=\"middle\" x=\"95.38\" y=\"-203.78\" font-family=\"Times,serif\" font-size=\"14.00\">C</text>\n",
       "<text text-anchor=\"middle\" x=\"95.38\" y=\"-187.28\" font-family=\"Times,serif\" font-size=\"14.00\">T1</text>\n",
       "<text text-anchor=\"middle\" x=\"95.38\" y=\"-170.78\" font-family=\"Times,serif\" font-size=\"14.00\">T2</text>\n",
       "<text text-anchor=\"middle\" x=\"95.38\" y=\"-154.28\" font-family=\"Times,serif\" font-size=\"14.00\">T3</text>\n",
       "<text text-anchor=\"middle\" x=\"95.38\" y=\"-137.78\" font-family=\"Times,serif\" font-size=\"14.00\">T4</text>\n",
       "<text text-anchor=\"middle\" x=\"95.38\" y=\"-121.28\" font-family=\"Times,serif\" font-size=\"14.00\">T5</text>\n",
       "<text text-anchor=\"middle\" x=\"95.38\" y=\"-104.78\" font-family=\"Times,serif\" font-size=\"14.00\">T6</text>\n",
       "</g>\n",
       "<!-- BERT&#45;&gt;OUTPUT -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>BERT&#45;&gt;OUTPUT</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M95.38,-286.81C95.38,-279.04 95.38,-269.22 95.38,-258.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"98.88,-258.56 95.38,-248.56 91.88,-258.56 98.88,-258.56\"/>\n",
       "</g>\n",
       "<!-- TRAIN -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>TRAIN</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"184.75,-36 0,-36 0,0 190.75,0 190.75,-30 184.75,-36\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"184.75,-36 184.75,-30\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"190.75,-30 184.75,-30\"/>\n",
       "<text text-anchor=\"middle\" x=\"95.38\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">Loss: T4 should be the word &#39;of&#39;</text>\n",
       "</g>\n",
       "<!-- OUTPUT&#45;&gt;TRAIN -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>OUTPUT&#45;&gt;TRAIN</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" d=\"M95.38,-71.67C95.38,-63.02 95.38,-54.75 95.38,-47.42\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"98.88,-47.6 95.38,-37.6 91.88,-47.6 98.88,-47.6\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x140a67d60>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "# Cria o grafo no formato PNG\n",
    "dot = Digraph(format='png')\n",
    "\n",
    "# Subgrafo de Inputs\n",
    "with dot.subgraph(name='cluster_inputs') as c:\n",
    "    c.attr(label='Inputs', style='rounded,filled', color='lightgrey')\n",
    "    c.node('INPUT', '[CLS]\\nremove\\nsome\\nparts\\n[MASK]\\na\\nsentence')\n",
    "\n",
    "# N√≥ central: BERT\n",
    "dot.node('BERT', 'BERT')\n",
    "\n",
    "# Subgrafo de Outputs\n",
    "with dot.subgraph(name='cluster_outputs') as c:\n",
    "    c.attr(label='Outputs', style='rounded,filled', color='lightgrey')\n",
    "    c.node('OUTPUT', 'C\\nT1\\nT2\\nT3\\nT4\\nT5\\nT6')\n",
    "\n",
    "# N√≥ de Treinamento\n",
    "dot.node('TRAIN', 'Loss: T4 should be the word \\'of\\'', shape='note')\n",
    "\n",
    "# Conex√µes\n",
    "dot.edge('INPUT', 'BERT')\n",
    "dot.edge('BERT', 'OUTPUT')\n",
    "dot.edge('OUTPUT', 'TRAIN', style='dashed')\n",
    "\n",
    "# Exibe o diagrama\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39cb2ea",
   "metadata": {},
   "source": [
    "O diagrama mostra de forma esquem√°tica como funciona uma etapa de **Masked Language Modeling** usando o BERT:\n",
    "\n",
    "1. **Cluster ‚ÄúInputs‚Äù**  \n",
    "   - Cont√©m a sequ√™ncia de tokens que chega ao modelo:  \n",
    "     ```  \n",
    "     [CLS]  \n",
    "     remove  \n",
    "     some  \n",
    "     parts  \n",
    "     [MASK]  \n",
    "     a  \n",
    "     sentence  \n",
    "     ```  \n",
    "   - Aqui:  \n",
    "     - `[CLS]` √© o token especial usado pelo BERT para sinalizar o in√≠cio da entrada e cujo embedding pode ser usado em tarefas de classifica√ß√£o de sequ√™ncia.  \n",
    "     - Cada palavra (sub‚Äëpalavra) do texto vira um token separado.  \n",
    "     - O token `[MASK]` substitui a palavra que queremos que o modelo aprenda a prever (‚Äúof‚Äù, neste exemplo).\n",
    "\n",
    "2. **Seta para ‚ÄúBERT‚Äù**  \n",
    "   - A seta indica que esses tokens s√£o processados **simultaneamente** pelo encoder bidirecional do BERT.  \n",
    "   - Internamente, cada token √© convertido em um vetor de embedding, somado aos embeddings de posi√ß√£o e de segmento, e ent√£o passa por v√°rias camadas de aten√ß√£o e feed‚Äëforward.\n",
    "\n",
    "3. **Cluster ‚ÄúOutputs‚Äù**  \n",
    "   - Ap√≥s o processamento, o BERT gera um vetor de sa√≠da para **cada** token da sequ√™ncia de entrada:\n",
    "     ```  \n",
    "     C  \n",
    "     T1  \n",
    "     T2  \n",
    "     T3  \n",
    "     T4  \n",
    "     T5  \n",
    "     T6  \n",
    "     ```  \n",
    "   - Esses vetores representam, em dimens√µes altas, a informa√ß√£o contextualizada de cada posi√ß√£o.  \n",
    "   - Em um pipeline de ‚Äúfill‚Äëmask‚Äù, o vetor na posi√ß√£o de `[MASK]` (aqui, T4) √© passado por uma camada linear + softmax para produzir uma distribui√ß√£o de probabilidade sobre todo o vocabul√°rio.\n",
    "\n",
    "4. **Seta tracejada at√© ‚ÄúLoss: T4 should be the word 'of'‚Äù**  \n",
    "   - A linha tracejada conecta a sa√≠da do token mascarado (T4) ao crit√©rio de perda.  \n",
    "   - Durante o treinamento, compara‚Äëse o vetor T4 com a etiqueta ‚Äúof‚Äù usando Cross‚ÄëEntropy Loss:  \n",
    "     $$\n",
    "       \\mathcal{L} = -\\log P(\\text{‚Äúof‚Äù}\\mid \\text{contexto})\n",
    "     $$  \n",
    "   - O objetivo √© ajustar os pesos do BERT para que, dado o contexto completo, a probabilidade estimada para a palavra ‚Äúof‚Äù seja a maior.\n",
    "\n",
    "---\n",
    "\n",
    "**Em resumo**, o diagrama ilustra:\n",
    "\n",
    "- Como a sequ√™ncia de tokens de entrada, com um elemento mascarado, √© injetada no BERT.  \n",
    "- Como o BERT produz uma sa√≠da vetorial para cada posi√ß√£o.  \n",
    "- Como se define uma fun√ß√£o de perda que for√ßa o vetor correspondente √† posi√ß√£o mascarada a identificar corretamente a palavra que falta.  \n",
    "\n",
    "Esse fluxo √© a ess√™ncia do **Masked Language Modeling**, permitindo que o BERT aprenda representa√ß√µes ricas e bidirecionais ao prever palavras ausentes usando todo o contexto ao redor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9324917e",
   "metadata": {},
   "source": [
    "### Exemplo de uso do pipeline `fill-mask` com BERT\n",
    "\n",
    "A seguir, usamos o pipeline de **Masked Language Modeling** (`fill-mask`) para preencher o token mascarado em uma frase:\n",
    "\n",
    "1. **Cria√ß√£o do pipeline**  \n",
    "   ```python\n",
    "   from transformers import pipeline\n",
    "   unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "\n",
    "\t2.\tAplica√ß√£o\n",
    "Ao chamar:\n",
    "\n",
    "unmasker(\"Remove some parts [MASK] a sentence.\")\n",
    "\n",
    "o pipeline retorna uma lista (top‚Äë5 por padr√£o) de poss√≠veis substitui√ß√µes para [MASK], cada uma com:\n",
    "\t‚Ä¢\tscore: probabilidade estimada pelo modelo.\n",
    "\t‚Ä¢\ttoken: ID do token no vocabul√°rio.\n",
    "\t‚Ä¢\ttoken_str: string do token previsto.\n",
    "\t‚Ä¢\tsequence: frase completa com o token substitu√≠do.\n",
    "\n",
    "Interpreta√ß√£o do resultado:\n",
    "\t‚Ä¢\tA melhor previs√£o para preencher [MASK] foi ‚Äúof‚Äù com probabilidade ~0.9431.\n",
    "\t‚Ä¢\tAs demais s√£o sugest√µes ordenadas por confian√ßa decrescente.\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "# 1) Cria o pipeline de Masked LM\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "\n",
    "# 2) Testa com a frase mascarada\n",
    "results = unmasker(\"Remove some parts [MASK] a sentence.\")\n",
    "\n",
    "# 3) Imprime cada sugest√£o\n",
    "for r in results:\n",
    "    print(f\"Token: {r['token_str']:<7}  Score: {r['score']:.4f}  Sequence: {r['sequence']}\")\n",
    "\n",
    "Token: of       0.9431  Sequence: remove some parts of a sentence.\n",
    "Token: from     0.0499  Sequence: remove some parts from a sentence.\n",
    "Token: in       0.0042  Sequence: remove some parts in a sentence.\n",
    "Token: within   0.0006  Sequence: remove some parts within a sentence.\n",
    "Token: during   0.0005  Sequence: remove some parts during a sentence.\n",
    "\n",
    "Ôøº"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da67423b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9431219100952148,\n",
       "  'token': 1997,\n",
       "  'token_str': 'of',\n",
       "  'sequence': 'remove some parts of a sentence.'},\n",
       " {'score': 0.04985547065734863,\n",
       "  'token': 2013,\n",
       "  'token_str': 'from',\n",
       "  'sequence': 'remove some parts from a sentence.'},\n",
       " {'score': 0.0042089857161045074,\n",
       "  'token': 1999,\n",
       "  'token_str': 'in',\n",
       "  'sequence': 'remove some parts in a sentence.'},\n",
       " {'score': 0.0006226670229807496,\n",
       "  'token': 2306,\n",
       "  'token_str': 'within',\n",
       "  'sequence': 'remove some parts within a sentence.'},\n",
       " {'score': 0.0005233804695308208,\n",
       "  'token': 2076,\n",
       "  'token_str': 'during',\n",
       "  'sequence': 'remove some parts during a sentence.'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "unmasker(\"Remove some parts [MASK] a sentence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868252d0",
   "metadata": {},
   "source": [
    "### Vi√©s Algor√≠tmico e Alucina√ß√µes\n",
    "\n",
    "Note que o BERT est√° gerando palavras que **fazem sentido** lingu√≠stico, mas essas continua√ß√µes **nem sempre correspondem √† realidade**‚Äîs√£o apenas as palavras que **maximizam a probabilidade** segundo o corpus de treino.\n",
    "\n",
    "**Exemplo**:  \n",
    "```python\n",
    "unmasker(\"Kentucky is famous for its [MASK].\")\n",
    "\n",
    "Sa√≠da (top‚Äë5 por padr√£o):\n",
    "\n",
    "[{'score': 0.0757, 'token_str': 'wine',    'sequence': 'kentucky is famous for its wine.'},\n",
    " {'score': 0.0674, 'token_str': 'wines',   'sequence': 'kentucky is famous for its wines.'},\n",
    " {'score': 0.0282, 'token_str': 'beaches', 'sequence': 'kentucky is famous for its beaches.'},\n",
    " {'score': 0.0228, 'token_str': 'cuisine', 'sequence': 'kentucky is famous for its cuisine.'},\n",
    " {'score': 0.0212, 'token_str': 'horses',  'sequence': 'kentucky is famous for its horses.'}]\n",
    "\n",
    "Explica√ß√£o aprofundada:\n",
    "\t‚Ä¢\tO modelo aprendeu a probabilidade de cada palavra aparecer ap√≥s ‚ÄúKentucky is famous for its‚Äù no corpus de pr√©‚Äëtreino, que cont√©m muitos textos sobre wine, distillery, etc.\n",
    "\t‚Ä¢\tEle n√£o ‚Äúsabe‚Äù fatos verdadeiros sobre Kentucky; est√° apenas reproduzindo padr√µes estat√≠sticos.\n",
    "\t‚Ä¢\tQuando o modelo produz sequ√™ncias inver√≠dicas ou implaus√≠veis, chamamos isso de alucina√ß√£o.\n",
    "\t‚Ä¢\tAl√©m disso, se o corpus tiver vi√©s (por ex. mais textos sobre vinho), o modelo ir√° refletir e at√© amplificar esse vi√©s ‚Äî √© o vi√©s algor√≠tmico.\n",
    "\n",
    "Em suma, BERT e outros LMs n√£o garantem veracidade, apenas otimizam a probabilidade de ocorr√™ncia de uma sequ√™ncia segundo os dados em que foram treinados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b84487",
   "metadata": {},
   "source": [
    "## Vi√©s Algor√≠tmico e Alucina√ß√µes (continua√ß√£o)\n",
    "\n",
    "Agora, substitua **‚Äúman‚Äù** por **‚Äúwoman‚Äù**. Voc√™ ver√° que o resultado n√£o √© t√£o satisfat√≥rio. Esse problema **n√£o** est√° na arquitetura do modelo, mas sim nos **dados** usados no pr√©‚Äëtreino.\n",
    "\n",
    "Podemos encontrar in√∫meros exemplos de outros tipos de preconceito ‚Äî existem vieses de g√™nero, ra√ßa e muito mais **ocultos** nos espa√ßos de embedding do BERT.\n",
    "\n",
    "Isso √© preocupante, mas lembre-se: em 2019 as pessoas ficaram impressionadas simplesmente pelo fato de o sistema gerar palavras coerentes! Hoje em dia, as sa√≠das de LLMs geralmente passam por **mecanismos de filtragem** que identificam e bloqueiam frases potencialmente nocivas, evitando respostas ofensivas.\n",
    "\n",
    "---\n",
    "\n",
    "### Exerc√≠cio\n",
    "\n",
    "Quais das seguintes afirma√ß√µes s√£o **verdadeiras** a respeito desse cen√°rio?\n",
    "\n",
    "1. O problema de vi√©s √© inerente √† arquitetura do BERT.  \n",
    "2. Os vieses refletem falhas nos dados de treinamento.  \n",
    "3. Os filtros atuais de LLMs eliminam completamente qualquer forma de preconceito.  \n",
    "4. O uso de `[MASK]` emularia o vi√©s independentemente do modelo.  \n",
    "5. Compartilhar modelos pr√©‚Äëtreinados ajuda a diminuir o vi√©s coletivo.\n",
    "\n",
    "Para cada item, justifique sua resposta com base no que discutimos sobre **dados**, **modelo** e **ferramentas de p√≥s‚Äëprocessamento**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ee5f27",
   "metadata": {},
   "source": [
    "## Respostas ao Exerc√≠cio de Vi√©s e Filtragem em LLMs\n",
    "\n",
    "1. O problema de vi√©s √© inerente √† arquitetura do BERT.  \n",
    "**Resposta:** **FALSO**  \n",
    "**Justificativa:**  \n",
    "- A arquitetura Transformer (encoder‚Äëonly) do BERT **n√£o imp√µe** vi√©s de g√™nero, ra√ßa ou religi√£o.  \n",
    "- O vi√©s emerge **dos dados** utilizados no pr√©‚Äëtreino (textos na web, livros, artigos), que carregam estere√≥tipos humanos.  \n",
    "- **Exemplo em c√≥digo:** veja como o pipeline de preenchimento produz sa√≠das diferentes para frases similares com ‚Äúman‚Äù e ‚Äúwoman‚Äù:\n",
    "  ```python\n",
    "  from transformers import pipeline\n",
    "  unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "\n",
    "  # Preenchendo para ‚Äúman‚Äù\n",
    "  print(unmasker(\"He is a [MASK].\")[0]['token_str'])  \n",
    "  # ‚Üí 'man' pode resultar em: ['man', 'developer', 'doctor', ...]\n",
    "\n",
    "  # Preenchendo para ‚Äúwoman‚Äù\n",
    "  print(unmasker(\"She is a [MASK].\")[0]['token_str']) \n",
    "  # ‚Üí 'woman' pode resultar em: ['woman', 'nurse', 'teacher', ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b28a8802",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "christian\n",
      "christian\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "\n",
    "# Preenchendo para ‚Äúman‚Äù\n",
    "print(unmasker(\"He is a [MASK].\")[0]['token_str'])  \n",
    "# ‚Üí 'man' pode resultar em: ['man', 'developer', 'doctor', ...]\n",
    "\n",
    "# Preenchendo para ‚Äúwoman‚Äù\n",
    "print(unmasker(\"She is a [MASK].\")[0]['token_str']) \n",
    "# ‚Üí 'woman' pode resultar em: ['woman', 'nurse', 'teacher', ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8018cc",
   "metadata": {},
   "source": [
    "2. Os vieses refletem falhas nos dados de treinamento.\n",
    "\n",
    "Resposta: VERDADEIRO\n",
    "Justificativa:\n",
    "- Modelos de linguagem aprendem estat√≠sticas de co‚Äëocorr√™ncia e frequ√™ncia.\n",
    "- Se o dataset de pr√©‚Äëtreino associa ‚Äúnurse‚Äù mais frequentemente com ‚Äúwoman‚Äù do que com ‚Äúman‚Äù, o modelo reproduz esse estere√≥tipo.\n",
    "- Para mitigar, podemos:\n",
    "- Filtrar corpora antes do pr√©‚Äëtreino (remover textos preconceituosos).\n",
    "- Reamostrar dados para equilibrar representa√ß√µes de grupos.\n",
    "- Fine‚Äëtunar com dados contrabalanceados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4d511c",
   "metadata": {},
   "source": [
    "3. Os filtros atuais de LLMs eliminam completamente qualquer forma de preconceito.\n",
    "\n",
    "Resposta: FALSO\n",
    "Justificativa:\n",
    "- Filtros de p√≥s‚Äëprocessamento (toxicity checkers, regras de modera√ß√£o) reduzem a gera√ß√£o de conte√∫do ofensivo, mas n√£o removem todos os vieses sutis.\n",
    "- Vieses de associa√ß√£o (por ex., ‚ÄúCEO ‚Üí man‚Äù) muitas vezes passam despercebidos por sistemas de filtragem.\n",
    "- Exemplos de vieses sutis:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1a3a01",
   "metadata": {},
   "source": [
    "4. O uso de [MASK] emularia o vi√©s independentemente do modelo.\n",
    "\n",
    "Resposta: FALSO\n",
    "Justificativa:\n",
    "- O token [MASK] √© apenas uma m√°scara de posi√ß√£o ‚Äî a mec√¢nica de preenchimento depende sempre dos embeddings e do treinamento do modelo espec√≠fico.\n",
    "- Se tiv√©ssemos um modelo livre de vi√©s, usar [MASK] n√£o geraria estere√≥tipos.\n",
    "- A posi√ß√£o [MASK] s√≥ revela o vi√©s que j√° existe no espa√ßo de embedding do modelo, n√£o o gera."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e501891",
   "metadata": {},
   "source": [
    "5. Compartilhar modelos pr√©‚Äëtreinados ajuda a diminuir o vi√©s coletivo.\n",
    "\n",
    "Resposta: VERDADEIRO (com ressalvas)\n",
    "Justificativa:\n",
    "- Quando comunidades abertas disponibilizam pesos e tokenizers, pesquisadores podem:\n",
    "- Auditar vieses (medir analogias enviesadas, disparidades de ra√ßa/g√™nero).\n",
    "- Propor corre√ß√µes (fine‚Äëtuning, distila√ß√£o consciente de vieses).\n",
    "- Exemplo de colabora√ß√£o:\n",
    "- Projetos como HateCheck usam modelos p√∫blicos para avaliar e propor patches de vi√©s.\n",
    "- Ressalva: apenas compartilhar n√£o basta; √© preciso ferramentas e boas pr√°ticas para corrigir e reavaliar continuamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232ac0f2",
   "metadata": {},
   "source": [
    "## Conclus√£o:\n",
    "- Arquitetura do BERT √© neutra; o vi√©s vem dos dados.\n",
    "- Filtros atenuam piora, mas n√£o eliminam vieses profundos.\n",
    "- O token [MASK] apenas exp√µe o vi√©s do modelo pr√©‚Äëtreinado.\n",
    "- A colabora√ß√£o aberta (compartilhar modelos + auditorias) √© essencial para identificar e mitigar vi√©s em larga escala."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4239db37",
   "metadata": {},
   "source": [
    "## Tarefa 2: Next Sentence Prediction (NSP)\n",
    "\n",
    "O BERT tamb√©m √© pr√©‚Äëtreinado para a tarefa **Next Sentence Prediction**. Nela, concatenamos duas frases na entrada, separadas pelo token `[SEP]`, e usamos o vetor do token `[CLS]` (denotado como \\(C\\)) para classificar se a segunda frase realmente segue a primeira.\n",
    "\n",
    "A ideia √©:\n",
    "\n",
    "1. **Exemplo positivo**  \n",
    "   ```text\n",
    "   [CLS] Here I am [SEP] rock you like a hurricane\n",
    "\n",
    "‚Üí o r√≥tulo (‚ÄúIsNext‚Äù) deve ser 1.\n",
    "\t2.\tExemplo negativo\n",
    "\n",
    "[CLS] Here I am [SEP] rock your body\n",
    "\n",
    "‚Üí o r√≥tulo (‚ÄúIsNext‚Äù) deve ser 0.\n",
    "\n",
    "Ap√≥s o pr√©‚Äëtreino, o vetor (C) captura a rela√ß√£o entre as duas senten√ßas, permitindo us√°‚Äëlo como feature em tarefas de classifica√ß√£o de sequ√™ncia.\n",
    "\n",
    "‚∏ª\n",
    "\n",
    "Usando a Hugging Face para NSP\n",
    "\n",
    "Podemos usar a classe AutoModelForNextSentencePrediction e o AutoTokenizer da biblioteca Transformers para executar NSP diretamente:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff81d73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positivo (esperado IsNext ~1): {'IsNext': 0.9918953776359558, 'NotNext': 0.008104616776108742}\n",
      "Negativo (esperado NotNext ~1): {'IsNext': 0.04612401872873306, 'NotNext': 0.9538760185241699}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForNextSentencePrediction\n",
    "import torch\n",
    "\n",
    "# 1) Carregue o tokenizer e o modelo (inclui a cabe√ßa NSP)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model     = AutoModelForNextSentencePrediction.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Fun√ß√£o auxiliar para avaliar NSP\n",
    "def is_next_sentence(sent_a, sent_b):\n",
    "    # Tokeniza as duas senten√ßas, inserindo [CLS] ... [SEP] ... [SEP]\n",
    "    inputs = tokenizer(sent_a, sent_b, return_tensors=\"pt\")\n",
    "    # Obt√©m os logits e converte em probabilidades\n",
    "    logits = model(**inputs).logits  # shape [1, 2]: [IsNext, NotNext]\n",
    "    probs  = torch.softmax(logits, dim=1).squeeze().tolist()\n",
    "    return {\"IsNext\": probs[0], \"NotNext\": probs[1]}\n",
    "\n",
    "# 2) Teste com um par de senten√ßas que realmente seguem\n",
    "result_pos = is_next_sentence(\n",
    "    \"Here I am\", \n",
    "    \"rock you like a hurricane\"\n",
    ")\n",
    "print(\"Positivo (esperado IsNext ~1):\", result_pos)\n",
    "\n",
    "# 3) Teste com um par de senten√ßas que N√ÉO seguem\n",
    "result_neg = is_next_sentence(\n",
    "    \"Here I am\", \n",
    "    \"rock your body\"\n",
    ")\n",
    "print(\"Negativo (esperado NotNext ~1):\", result_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5444568",
   "metadata": {},
   "source": [
    "\n",
    "Explica√ß√£o dos passos:\n",
    " 1. Tokeniza√ß√£o:\n",
    "- tokenizer(sent_a, sent_b, return_tensors=\"pt\") insere automaticamente os tokens [CLS], [SEP] e segment embeddings para cada senten√ßa.\n",
    " 2. Infer√™ncia:\n",
    "- model(**inputs).logits retorna um tensor de forma [batch_size, 2] contendo os logits para as classes IsNext (√≠ndice¬†0) e NotNext (√≠ndice¬†1).\n",
    " 3. Probabilidades:\n",
    "- Aplicamos softmax sobre os logits para obter probabilidades, e assim podemos verificar qual das duas classes o modelo considera mais prov√°vel.\n",
    "\n",
    "Dessa forma, utilizamos o pr√©‚Äëtreino NSP do BERT para decidir se uma frase segue logicamente outra, extraindo do token [CLS] um vetor de classifica√ß√£o que representa a rela√ß√£o entre as senten√ßas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f388a3f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5db448f2",
   "metadata": {},
   "source": [
    "### Carregando o Tokenizer e o Modelo BERT\n",
    "\n",
    "Nesta etapa, vamos carregar o tokenizer e o modelo pr√©‚Äëtreinado `bert-base-uncased` da Hugging Face:\n",
    "\n",
    "- **Tokenizer**: converte texto em IDs de tokens (WordPieces) e gera tensores prontos para o modelo.  \n",
    "- **Modelo**: `BertModel` retorna, para cada token, um vetor contextualizado de dimens√£o igual ao hidden size (geralmente 768 em `bert-base-uncased`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79f2a97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# 1) Carrega o tokenizer e o modelo pr√©‚Äëtreinado\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model     = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 2) Prepara um texto qualquer\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "\n",
    "# 3) Tokeniza e retorna tensores PyTorch\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "# 4) Executa o modelo (torch.no_grad() pode ser usado em infer√™ncia)\n",
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550c78e9",
   "metadata": {},
   "source": [
    "Explica√ß√£o detalhada:\n",
    "- encoded_input √© um dicion√°rio com chaves como input_ids e attention_mask, ambos tensores de shape [batch_size, seq_len].\n",
    "- output √© um objeto do tipo BaseModelOutputWithPoolingAndCrossAttentions contendo:\n",
    "- last_hidden_state: tensor [batch_size, seq_len, hidden_size] com os embeddings de cada token.\n",
    "- pooler_output: tensor [batch_size, hidden_size] que √© uma proje√ß√£o do embedding do token [CLS]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed3dd46",
   "metadata": {},
   "source": [
    "## Extraindo o Embedding do Token [CLS]\n",
    "\n",
    "Para usar o vetor que representa toda a sequ√™ncia (√∫til em classifica√ß√£o), basta capturar a primeira posi√ß√£o de last_hidden_state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8f11f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'output.last_hidden_state' tem shape [1, seq_len, hidden_size]\n",
    "# [0,0,:] significa: batch index 0, token index 0 ([CLS]), todos os hidden units\n",
    "output_cls = output.last_hidden_state[0, 0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f7976a",
   "metadata": {},
   "source": [
    "Dica de atalho:\n",
    "- Voc√™ tamb√©m pode usar diretamente output.pooler_output, que j√° √© o embedding resultante do [CLS] passado por uma camada adicional de ativa√ß√£o."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e25fb13",
   "metadata": {},
   "source": [
    "### Explora√ß√£o completa da gera√ß√£o de embeddings no BERT\n",
    "\n",
    "O BERT gera embeddings em v√°rias etapas bem definidas. Abaixo, detalhamos cada componente e mostramos como o modelo os combina internamente.\n",
    "\n",
    "1. **Tokeniza√ß√£o**  \n",
    "   - O texto √© dividido em *WordPieces* e transformado em IDs de tokens (`input_ids`).  \n",
    "   - S√£o gerados tamb√©m:\n",
    "     - `token_type_ids` (segment embeddings): indicam se cada token pertence √† senten√ßa A ou B.  \n",
    "     - `attention_mask`: m√°scara para ignorar padding.  \n",
    "\n",
    "2. **Camada de Embedding**  \n",
    "   - **Word embeddings**:  \n",
    "     ```python\n",
    "     word_emb = model.embeddings.word_embeddings(input_ids)\n",
    "     ```\n",
    "   - **Position embeddings**:  \n",
    "     ```python\n",
    "     # Cria position_ids = [[0,1,2,...], [0,1,2,...], ...]\n",
    "     position_ids = torch.arange(input_ids.size(1)).unsqueeze(0).expand(input_ids.size())\n",
    "     pos_emb = model.embeddings.position_embeddings(position_ids)\n",
    "     ```\n",
    "   - **Token type embeddings**:  \n",
    "     ```python\n",
    "     token_type_emb = model.embeddings.token_type_embeddings(token_type_ids)\n",
    "     ```\n",
    "   - **Soma dos tr√™s embeddings**:\n",
    "     ```python\n",
    "     inputs_embeds = word_emb + pos_emb + token_type_emb\n",
    "     ```\n",
    "   - **LayerNorm e Dropout**:\n",
    "     ```python\n",
    "     inputs_embeds = model.embeddings.LayerNorm(inputs_embeds)\n",
    "     inputs_embeds = model.embeddings.dropout(inputs_embeds)\n",
    "     ```\n",
    "\n",
    "3. **Encoder (Transformer Layers)**  \n",
    "   - Em `bert-base-uncased`, s√£o 12 camadas de encoder, cada uma com:\n",
    "     - *Multi‚ÄëHead Self‚ÄëAttention* (8 cabe√ßas)  \n",
    "     - *Feed-Forward* (hidden_size‚Üí4√óhidden_size‚Üíhidden_size)  \n",
    "     - *Residual + LayerNorm* em cada subcamada  \n",
    "   - C√≥digo aproximado:\n",
    "     ```python\n",
    "     encoder_outputs = model.encoder(\n",
    "         inputs_embeds,\n",
    "         attention_mask=attention_mask\n",
    "     )\n",
    "     last_hidden_state = encoder_outputs.last_hidden_state\n",
    "     # shape: [batch_size, seq_len, hidden_size]\n",
    "     ```\n",
    "\n",
    "4. **Pooler (Embedding de senten√ßa)**  \n",
    "   - Seleciona o vetor do token `[CLS]` (posi√ß√£o 0):  \n",
    "     ```python\n",
    "     cls_token_emb = last_hidden_state[:, 0, :]  # [batch_size, hidden_size]\n",
    "     ```\n",
    "   - Aplica uma camada linear + `tanh`:  \n",
    "     ```python\n",
    "     pooler_output = model.pooler(cls_token_emb)\n",
    "     # shape: [batch_size, hidden_size]\n",
    "     ```\n",
    "\n",
    "5. **Sa√≠das**  \n",
    "   - `last_hidden_state` ‚Üí embeddings contextuais para **cada** token.  \n",
    "   - `pooler_output`     ‚Üí embedding de **toda** a sequ√™ncia (√∫til em classifica√ß√£o downstream).\n",
    "\n",
    "---\n",
    "\n",
    "Abaixo, o c√≥digo completo mostrando cada etapa:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# 1) Carrega tokenizer e modelo\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model     = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 2) Exemplo de batch de senten√ßas\n",
    "texts = [\"Hello world!\", \"How are you doing?\"]\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "input_ids      = inputs['input_ids']       # [batch_size, seq_len]\n",
    "token_type_ids = inputs.get('token_type_ids', torch.zeros_like(input_ids))\n",
    "attention_mask = inputs['attention_mask']\n",
    "\n",
    "# 3) WordPiece embeddings\n",
    "word_emb = model.embeddings.word_embeddings(input_ids)\n",
    "\n",
    "# 4) Position embeddings\n",
    "position_ids = torch.arange(input_ids.size(1)).unsqueeze(0).expand(input_ids.size())\n",
    "pos_emb = model.embeddings.position_embeddings(position_ids)\n",
    "\n",
    "# 5) Token type embeddings\n",
    "token_type_emb = model.embeddings.token_type_embeddings(token_type_ids)\n",
    "\n",
    "# 6) Soma + LayerNorm + Dropout\n",
    "inputs_embeds = word_emb + pos_emb + token_type_emb\n",
    "inputs_embeds = model.embeddings.LayerNorm(inputs_embeds)\n",
    "inputs_embeds = model.embeddings.dropout(inputs_embeds)\n",
    "\n",
    "# 7) Passa pelo encoder (todas as camadas Transformer)\n",
    "encoder_outputs     = model.encoder(inputs_embeds, attention_mask=attention_mask)\n",
    "last_hidden_state   = encoder_outputs.last_hidden_state  # [batch, seq_len, hidden_size]\n",
    "\n",
    "# 8) Pooler: embedding de sequ√™ncia via [CLS]\n",
    "cls_token_emb  = last_hidden_state[:, 0, :]        # [batch, hidden_size]\n",
    "pooler_output  = model.pooler(cls_token_emb)       # [batch, hidden_size]\n",
    "\n",
    "# 9) Verifica shapes\n",
    "print(\"last_hidden_state shape:\", last_hidden_state.shape)\n",
    "print(\"pooler_output    shape:\", pooler_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed5780f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_hidden_state shape: torch.Size([2, 7, 768])\n",
      "pooler_output    shape: torch.Size([2, 768])\n"
     ]
    }
   ],
   "source": [
    "# 1) Carrega tokenizer e modelo\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model     = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 2) Tokeniza\n",
    "texts = [\"Hello world!\", \"How are you doing?\"]\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# 3) Converte attention_mask para float ou bool\n",
    "inputs['attention_mask'] = inputs['attention_mask'].to(dtype=torch.float)  # ou .bool()\n",
    "\n",
    "# 4) Monta embeddings de entrada manualmente (opcional)\n",
    "word_emb       = model.embeddings.word_embeddings(inputs['input_ids'])\n",
    "position_ids   = torch.arange(inputs['input_ids'].size(1)).unsqueeze(0).expand(inputs['input_ids'].size())\n",
    "pos_emb        = model.embeddings.position_embeddings(position_ids)\n",
    "token_type_emb = model.embeddings.token_type_embeddings(inputs.get('token_type_ids', torch.zeros_like(inputs['input_ids'])))\n",
    "\n",
    "inputs_embeds  = word_emb + pos_emb + token_type_emb\n",
    "inputs_embeds  = model.embeddings.LayerNorm(inputs_embeds)\n",
    "inputs_embeds  = model.embeddings.dropout(inputs_embeds)\n",
    "\n",
    "# 5) Chama o modelo completo ‚Äî ele faz a convers√£o interna da m√°scara\n",
    "outputs = model(\n",
    "    inputs_embeds=inputs_embeds,\n",
    "    attention_mask=inputs['attention_mask']\n",
    ")\n",
    "\n",
    "last_hidden_state = outputs.last_hidden_state  # [batch_size, seq_len, hidden_size]\n",
    "pooler_output     = outputs.pooler_output      # [batch_size, hidden_size]\n",
    "\n",
    "print(\"last_hidden_state shape:\", last_hidden_state.shape)\n",
    "print(\"pooler_output    shape:\", pooler_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c87331",
   "metadata": {},
   "source": [
    "## Exerc√≠cio: Classifica√ß√£o usando embeddings do BERT\n",
    "\n",
    "Em vez de usar TF‚ÄëIDF + Regress√£o Log√≠stica, podemos extrair **embeddings contextuais** de cada sinopse de filme usando um BERT pr√©‚Äëtreinado e ent√£o treinar um classificador sobre esses vetores:\n",
    "\n",
    "1. **Carregar dados e dividir em treino/teste** ‚Äî como antes.  \n",
    "2. **Tokenizar e gerar embeddings**:\n",
    "   - Usar `AutoTokenizer` + `AutoModel` para obter `pooler_output` (ou o embedding do token `[CLS]`) para cada texto.  \n",
    "   - Agregar num array de shape `(n_samples, hidden_size)`.\n",
    "\n",
    "3. **Treinar um classificador** (por exemplo, `LogisticRegression`) sobre esses embeddings.  \n",
    "4. **Avaliar** usando `classification_report`.\n",
    "\n",
    "Esse fluxo mostra o poder de embeddings pr√©‚Äëtreinados: sem engenharia de features, aproveitamos representa√ß√µes ricas aprendidas em corpora massivos.\n",
    "\n",
    "---\n",
    "\n",
    "### Passos gerais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a0465b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84167d7",
   "metadata": {},
   "source": [
    "1.\tCarregar e dividir os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a0df10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/tiagoft/NLP/main/wiki_movie_plots_drama_comedy.csv')\n",
    "df = df.sample(1000, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['Plot'], df['Genre'], test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62080bf",
   "metadata": {},
   "source": [
    "2.\tConfigurar BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b81410d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model     = AutoModel.from_pretrained('bert-base-uncased')\n",
    "model.eval()  # modo infer√™ncia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2975d5",
   "metadata": {},
   "source": [
    "3.\tFun√ß√£o para extrair embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46e7cb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(texts, batch_size=16, max_len=128):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size].tolist()\n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        # pooler_output: [batch_size, hidden_size]\n",
    "        embs = outputs.pooler_output.cpu().numpy()\n",
    "        embeddings.append(embs)\n",
    "    return np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e058959",
   "metadata": {},
   "source": [
    "4.\tGerar embeddings para treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "104377f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_emb = get_embeddings(X_train, batch_size=32)\n",
    "X_test_emb  = get_embeddings(X_test,  batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be33fcd6",
   "metadata": {},
   "source": [
    "5.\tTreinar e avaliar o classificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9649f72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      comedy       0.63      0.59      0.61        87\n",
      "       drama       0.70      0.73      0.72       113\n",
      "\n",
      "    accuracy                           0.67       200\n",
      "   macro avg       0.66      0.66      0.66       200\n",
      "weighted avg       0.67      0.67      0.67       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train_emb, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_emb)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e14efa3",
   "metadata": {},
   "source": [
    "Dessa forma, voc√™ compara diretamente o desempenho de um classificador baseado em embeddings contextuais do BERT com o pipeline cl√°ssico de TF‚ÄëIDF. Isso costuma melhorar a acur√°cia em tarefas de texto, sobretudo quando h√° ambiguidade ou rela√ß√µes sem√¢nticas complexas nos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bed406",
   "metadata": {},
   "source": [
    "### Classifica√ß√£o com Embeddings do BERT (Solu√ß√£o do Professor)\n",
    "\n",
    "A seguir, apresentamos um fluxo completo para:\n",
    "\n",
    "1. **Carregar e amostrar os dados**  \n",
    "2. **Pr√©‚Äëprocessar textos** e **extrair embeddings** do token `[CLS]` usando o BERT  \n",
    "3. **Salvar** e **recarregar** os embeddings em disco  \n",
    "4. **Dividir** em treino/teste  \n",
    "5. **Treinar** um classificador (`LogisticRegression`) sobre esses vetores  \n",
    "6. **Avaliar** o desempenho com `classification_report`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed7c78c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computando embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [02:14<00:00,  7.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      comedy       0.68      0.76      0.72        85\n",
      "       drama       0.81      0.74      0.77       115\n",
      "\n",
      "    accuracy                           0.75       200\n",
      "   macro avg       0.75      0.75      0.75       200\n",
      "weighted avg       0.76      0.75      0.75       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Passo 0: Carregar e amostrar os dados\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\n",
    "    'https://raw.githubusercontent.com/tiagoft/NLP/main/wiki_movie_plots_drama_comedy.csv'\n",
    ").sample(1000, random_state=42)\n",
    "\n",
    "X = df['Plot']    # sinopses de filmes\n",
    "y = df['Genre']   # drama ou comedy\n",
    "\n",
    "# Passo 1: Pr√©‚Äëprocessamento e extra√ß√£o de embeddings\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Carrega tokenizer e modelo BERT pr√©‚Äëtreinado\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model     = BertModel.from_pretrained('bert-base-uncased')\n",
    "model.eval()  # modo infer√™ncia (desativa dropout)\n",
    "\n",
    "def get_embeddings(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Retorna o embedding do token [CLS] para a string `text`.\n",
    "    \"\"\"\n",
    "    # Tokeniza, adicionando padding/truncation at√© max_length=512\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    # Inhibe c√°lculo de gradiente\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Pega o vetor correspondente ao [CLS] (posi√ß√£o 0)\n",
    "    cls_embedding = outputs.last_hidden_state[0, 0, :]\n",
    "    return cls_embedding\n",
    "\n",
    "# Gera embeddings para todas as sinopses\n",
    "embeddings = []\n",
    "for plot in tqdm(X, desc='Computando embeddings'):\n",
    "    emb = get_embeddings(plot, model, tokenizer)\n",
    "    embeddings.append(emb.numpy())\n",
    "\n",
    "# Converte em array NumPy e salva em disco\n",
    "embeddings = np.stack(embeddings)  # shape: [1000, hidden_size]\n",
    "np.save('bert_embeddings.npy', embeddings)\n",
    "\n",
    "# Passo 2: Recarrega embeddings (caso precise acelerar itera√ß√µes)\n",
    "embeddings = np.load('bert_embeddings.npy')\n",
    "\n",
    "# Passo 3: Divide em treino e teste\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    embeddings, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Passo 4: Treina o classificador\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Passo 5: Faz previs√µes e imprime relat√≥rio\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd95b5f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
